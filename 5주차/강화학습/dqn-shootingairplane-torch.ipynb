{"cells":[{"cell_type":"code","source":["!pip --q install gymnasium"],"metadata":{"id":"RlEuxYC4aL1e","executionInfo":{"status":"ok","timestamp":1721703697043,"user_tz":-540,"elapsed":22963,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2b2c84ce-6fde-440a-dcb8-e0c85fbb9f5d"},"id":"RlEuxYC4aL1e","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","source":[],"metadata":{"id":"qCuAZFLtgc8x","executionInfo":{"status":"ok","timestamp":1721703697044,"user_tz":-540,"elapsed":12,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"id":"qCuAZFLtgc8x","execution_count":1,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pB1iMfsZfygh","executionInfo":{"status":"ok","timestamp":1721703719864,"user_tz":-540,"elapsed":22830,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"67dce33d-8dde-4583-e84b-1fa3219ca56d"},"id":"pB1iMfsZfygh","execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nR5BqqOWfyYc","executionInfo":{"status":"ok","timestamp":1721703719865,"user_tz":-540,"elapsed":14,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"d2f9445a-bbcf-476c-8d38-b1fb2f3fc645"},"id":"nR5BqqOWfyYc","execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["drive  sample_data\n"]}]},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/시리즈/gym_examples/gym_examples ."],"metadata":{"id":"2ssDbVX1fyJC","executionInfo":{"status":"ok","timestamp":1721703721766,"user_tz":-540,"elapsed":1908,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"id":"2ssDbVX1fyJC","execution_count":4,"outputs":[]},{"cell_type":"code","source":["import gymnasium as gym\n","env = gym.make('gym_examples:gym_examples/ShootingAirplane-v0', render_mode='text')"],"metadata":{"id":"gOp1Ryl8f4wh","executionInfo":{"status":"ok","timestamp":1721703722594,"user_tz":-540,"elapsed":832,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"id":"gOp1Ryl8f4wh","execution_count":5,"outputs":[]},{"cell_type":"code","source":["obs, info = env.reset()"],"metadata":{"id":"Gti1YtU9gHSJ","executionInfo":{"status":"ok","timestamp":1721703722594,"user_tz":-540,"elapsed":8,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"id":"Gti1YtU9gHSJ","execution_count":6,"outputs":[]},{"cell_type":"markdown","id":"250b088e","metadata":{"id":"250b088e"},"source":["# DQN으로 Shooring Airplane Game 강화학습\n","\n","먼저 여러가지 설정 변수 정의"]},{"cell_type":"code","execution_count":7,"id":"be000272-b287-41c7-b5ed-1bd352af1a71","metadata":{"tags":[],"id":"be000272-b287-41c7-b5ed-1bd352af1a71","executionInfo":{"status":"ok","timestamp":1721703737558,"user_tz":-540,"elapsed":14970,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["import numpy as np\n","import gymnasium as gym\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as T\n","\n","# Configuration paramaters for the whole setup\n","seed = 42\n","gamma = 0.99  # Discount factor for past rewards\n","epsilon = 1.0  # Epsilon greedy parameter\n","epsilon_min = 0.1  # Minimum epsilon greedy parameter\n","epsilon_max = 1.0  # Maximum epsilon greedy parameter\n","epsilon_interval = epsilon_max - epsilon_min # Rate at which to reduce chance\n","                                             # of random action being taken\n","batch_size = 16  # Size of batch taken from replay buffer\n","max_steps_per_episode = 60\n","max_episodes = 7500"]},{"cell_type":"markdown","id":"82a3fe92","metadata":{"id":"82a3fe92"},"source":["### 게임 환경 설정\n","\n","상태(state) 정의\n","- 보드판의 모양: (8 * 8) 행렬 * 3 채널\n","- 채널 0: unseen\n","- 채널 1: hit\n","- 채녈 2: miss\n","\n","액션 정의\n","- 돌의 가능한 위치 (8 * 8 = 64)"]},{"cell_type":"markdown","id":"bfab303e","metadata":{"id":"bfab303e"},"source":["env에서 정의한 action_space, observation_space 의 모양 확인\n","- action_space: 3개의 값의 튜플 (벡터)\n","- observation_space: HWC 형태의 이미지 (마지막 축이 단일 값인 15 * 15 * 1 텐서) -> pytorch를 사용할 경우 적절히 1 * 15 * 15 텐서로 수정필요"]},{"cell_type":"code","execution_count":8,"id":"d3ba8e44","metadata":{"id":"d3ba8e44","executionInfo":{"status":"ok","timestamp":1721703737560,"user_tz":-540,"elapsed":42,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4ae3724a-d3b1-471b-d71d-365a0337ae3d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2,)"]},"metadata":{},"execution_count":8}],"source":["env.action_space.shape"]},{"cell_type":"code","source":[],"metadata":{"id":"yuEqYykDbl2u","executionInfo":{"status":"ok","timestamp":1721703737560,"user_tz":-540,"elapsed":34,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"id":"yuEqYykDbl2u","execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"id":"d4c3c91f","metadata":{"id":"d4c3c91f","executionInfo":{"status":"ok","timestamp":1721703737560,"user_tz":-540,"elapsed":33,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8b2c71d3-095b-4fe3-878b-5f8200f9a48f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 8, 1)"]},"metadata":{},"execution_count":9}],"source":["env.observation_space.shape"]},{"cell_type":"markdown","id":"5477cab4","metadata":{"id":"5477cab4"},"source":["### 네트워크 정의하기\n","\n","참고: Conv2d 파라미터\n","* in_channels (int) – Number of channels in the input image\n","* out_channels (int) – Number of channels produced by the convolution\n","* kernel_size (int or tuple) – Size of the convolving kernel\n","* stride (int or tuple, optional) – Stride of the convolution. Default: 1\n","* padding (int, tuple or str, optional) – Padding added to all four sides of the input. Default: 0\n","* padding_mode (str, optional) – 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'"]},{"cell_type":"code","execution_count":10,"id":"f85ef96c-cc48-426a-b2f3-12e002502bc9","metadata":{"tags":[],"id":"f85ef96c-cc48-426a-b2f3-12e002502bc9","executionInfo":{"status":"ok","timestamp":1721703737560,"user_tz":-540,"elapsed":29,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","num_actions = 64\n","\n","class QModel(nn.Module):\n","    def __init__(self, num_actions):\n","        super(QModel, self).__init__()\n","        self.dropout = nn.Dropout(p=0.3)\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding='same')\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding='same')\n","        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(1152, 512)\n","        self.fc2 = nn.Linear(512, num_actions)\n","\n","    def forward(self, x):\n","        x = nn.functional.relu(self.conv1(x))\n","        x = nn.functional.relu(self.conv2(x))\n","        x = self.dropout(x)\n","        x = nn.functional.relu(self.conv3(x))\n","        x = self.flatten(x)\n","        x = nn.functional.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        action = self.fc2(x)\n","        return action"]},{"cell_type":"markdown","id":"21a23e4c","metadata":{"id":"21a23e4c"},"source":["### 모델 빌딩 & 로스 및 최적화 계산기 만들기\n","\n","\n","1.   Q-value 를 예측하기 위한 모델\n","2.   미래의 리워드를 예상하는 타겟 모델\n","\n"]},{"cell_type":"code","execution_count":11,"id":"8a6ec4cd-dd1d-49cf-a50d-fb04628b0f7f","metadata":{"id":"8a6ec4cd-dd1d-49cf-a50d-fb04628b0f7f","executionInfo":{"status":"ok","timestamp":1721703737561,"user_tz":-540,"elapsed":29,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# The first model makes the predictions for Q-values which are used to\n","# make a action.\n","model = QModel(num_actions)\n","model.to(device)\n","\n","# Build a target model for the prediction of future rewards.\n","# The weights of a target model get updated every 10000 steps thus when the\n","# loss between the Q-values is calculated the target Q-value is stable.\n","model_target = QModel(num_actions)\n","model_target.to(device)\n","\n","loss_function = nn.SmoothL1Loss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)"]},{"cell_type":"code","execution_count":12,"id":"38d7c152","metadata":{"id":"38d7c152","executionInfo":{"status":"ok","timestamp":1721703737561,"user_tz":-540,"elapsed":29,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6729dd90-7945-4ad8-989e-f13d1a41a89f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":12}],"source":["device"]},{"cell_type":"markdown","id":"a859f550","metadata":{"id":"a859f550"},"source":["### Replay Buffer 정의"]},{"cell_type":"code","execution_count":13,"id":"8eed8ce4-5a53-457f-bc40-fda577b2ec29","metadata":{"id":"8eed8ce4-5a53-457f-bc40-fda577b2ec29","executionInfo":{"status":"ok","timestamp":1721703737561,"user_tz":-540,"elapsed":25,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["# Experience replay buffers\n","action_history = []\n","action_mask_history = []\n","state_history = []\n","state_next_history = []\n","rewards_history = []\n","done_history = []\n","episode_reward_history = []\n","running_reward = 0\n","episode_count = 0\n","frame_count = 0\n","\n","# Number of frames to take random action and observe output\n","epsilon_random_frames = 50000\n","# Number of frames for exploration\n","epsilon_greedy_frames = 200000.0\n","# Maximum replay length\n","# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n","max_memory_length = 500000\n","# Train the model after 4 actions\n","update_after_actions = 4\n","# How often to update the target network\n","update_target_network = 10000"]},{"cell_type":"markdown","id":"eda734a1","metadata":{"id":"eda734a1"},"source":["### 전처리\n","\n","- env가 리턴하는 observation은 일단 np.array이니 torch.tensor로 캐스팅\n","- env가 리턴하는 상태가 (8, 8, 1)의 HWC 이미지 텐서이므로 이를 (3, 15, 15)의 CHW 이미지로 변환\n","- One-hot 인코딩도 필요"]},{"cell_type":"code","execution_count":14,"id":"abf51fd4","metadata":{"id":"abf51fd4","executionInfo":{"status":"ok","timestamp":1721703737561,"user_tz":-540,"elapsed":24,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["# Function to preprocess the state\n","# note that player 1 = env player, player 2 = agent\n","def preprocess_state(env_observ):\n","    st = torch.from_numpy(env_observ).squeeze()\n","    st = st.to(torch.int64)\n","    st = torch.nn.functional.one_hot(st,num_classes=3)\n","    st = st.permute(2, 0, 1)\n","    return st.to(torch.float32)"]},{"cell_type":"markdown","id":"06726258","metadata":{"id":"06726258"},"source":["### 중간 테스트\n","\n","- env.reset로 board observation를 받아서\n","- preprocess_state로 input tensor로 바꾸어주고\n","- model로 forward computing"]},{"cell_type":"code","execution_count":15,"id":"26af50b6","metadata":{"id":"26af50b6","executionInfo":{"status":"ok","timestamp":1721703737561,"user_tz":-540,"elapsed":23,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["board, info = env.reset()"]},{"cell_type":"code","execution_count":16,"id":"13fad4bb","metadata":{"id":"13fad4bb","executionInfo":{"status":"ok","timestamp":1721703737561,"user_tz":-540,"elapsed":23,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9dc7f5d3-3c4b-4312-c3a7-8ef8642411e8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]]], dtype=uint8)"]},"metadata":{},"execution_count":16}],"source":["board"]},{"cell_type":"code","execution_count":17,"id":"4e1cace0","metadata":{"id":"4e1cace0","executionInfo":{"status":"ok","timestamp":1721703737561,"user_tz":-540,"elapsed":18,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f0a9546a-c757-4de1-81d9-93efcd37912e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)"]},"metadata":{},"execution_count":17}],"source":["info['action_mask']"]},{"cell_type":"code","execution_count":18,"id":"b9506c58","metadata":{"id":"b9506c58","executionInfo":{"status":"ok","timestamp":1721703738323,"user_tz":-540,"elapsed":30,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["state = preprocess_state(board)"]},{"cell_type":"code","execution_count":19,"id":"d75f0561","metadata":{"id":"d75f0561","executionInfo":{"status":"ok","timestamp":1721703738324,"user_tz":-540,"elapsed":30,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c218a907-ada8-4b9a-fc58-a6b1fec54d7d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1., 1., 1., 1.]],\n","\n","        [[0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.]],\n","\n","        [[0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.]]])"]},"metadata":{},"execution_count":19}],"source":["state"]},{"cell_type":"code","execution_count":20,"id":"a388e92e","metadata":{"id":"a388e92e","executionInfo":{"status":"ok","timestamp":1721703738324,"user_tz":-540,"elapsed":28,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"10ae6661-cf6e-4d64-d04f-ab69c5436e5e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 8, 8])"]},"metadata":{},"execution_count":20}],"source":["state.shape"]},{"cell_type":"code","execution_count":21,"id":"f111d273","metadata":{"id":"f111d273","executionInfo":{"status":"ok","timestamp":1721703738324,"user_tz":-540,"elapsed":20,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["with torch.no_grad():\n","    model_output = model(state.unsqueeze(0))"]},{"cell_type":"code","execution_count":22,"id":"43a4547b","metadata":{"id":"43a4547b","executionInfo":{"status":"ok","timestamp":1721703738324,"user_tz":-540,"elapsed":19,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dda570a0-a963-484a-9a96-4377149f33c8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 64])"]},"metadata":{},"execution_count":22}],"source":["model_output.shape"]},{"cell_type":"markdown","id":"61693680","metadata":{"id":"61693680"},"source":["### Epsilon-greedy 액션 선택 함수\n","\n","학습시 에피소드 생성하면서 사용 (주의: 입력은 batch axis 없음)\n","Epsilon 값이 클 수록 Q-value에 의한 행동이 아닌 랜덤으로 행동하게 됨"]},{"cell_type":"code","execution_count":23,"id":"f3f9bd3c","metadata":{"id":"f3f9bd3c","executionInfo":{"status":"ok","timestamp":1721703738324,"user_tz":-540,"elapsed":16,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["# Function to select an action\n","# model: the torch model to compuate action-state value (i.e., q-value)\n","# state: a torch tensor (3 x 8 x 8) of float32, which is output by preprocess_state\n","# mask: a 64-size array (np.array)\n","def get_greedy_epsilon(model, state, mask):\n","    global epsilon\n","\n","    #if frame_count < epsilon_random_frames or np.random.rand(1)[0] < epsilon:\n","    if np.random.rand(1)[0] < epsilon:\n","        action = np.random.choice([ i for i in range(num_actions) if mask[i] == 1 ])\n","    else:\n","        with torch.no_grad():\n","            # add a batch axis\n","            state_tensor = state.unsqueeze(0)\n","            # compute the q-values\n","            q_values = model(state_tensor)\n","            # select the q-values of valid actions\n","            action = torch.argmax(\n","                q_values.squeeze() + torch.from_numpy(mask) * 100., # trick to select a valid action\n","                dim=0)\n","\n","\n","            #valid_q = [ (i, q_values[0][i]) for i in range(64) if mask[i] == 1 ]\n","            # the action of maximum q-value\n","            #action, _ = max(valid_q, key=lambda e: e[1])\n","\n","    # decay epsilon\n","    epsilon -= epsilon_interval / epsilon_greedy_frames\n","    epsilon = max(epsilon, epsilon_min)\n","\n","    return action"]},{"cell_type":"code","execution_count":24,"id":"c411ecfd","metadata":{"id":"c411ecfd","executionInfo":{"status":"ok","timestamp":1721703738325,"user_tz":-540,"elapsed":17,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"af96da2e-cd85-4db9-c070-d4411e591bee"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{},"execution_count":24}],"source":["mask = np.zeros((64,), dtype=np.int64)\n","mask[12] = 1.\n","get_greedy_epsilon(model, state, mask)"]},{"cell_type":"markdown","id":"ea6728b0","metadata":{"id":"ea6728b0"},"source":["### Greedy 액션 선택 함수\n","\n","나중에 evaluation 시 사용"]},{"cell_type":"code","execution_count":25,"id":"8025c8a5","metadata":{"id":"8025c8a5","executionInfo":{"status":"ok","timestamp":1721703738325,"user_tz":-540,"elapsed":14,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["def get_greedy_action(model, state, mask):\n","    global epsilon\n","\n","    with torch.no_grad():\n","        state_tensor = state.unsqueeze(0) # batch dimension\n","        q_values = model(state_tensor)\n","\n","        action = torch.argmax(\n","                q_values.squeeze() + torch.from_numpy(mask) * 100., # trick to select a valid action\n","                dim=0)\n","\n","    return action"]},{"cell_type":"markdown","id":"b99ac400","metadata":{"id":"b99ac400"},"source":["### Update 파트\n","\n","- Replay buffer 에서 batch하나를 샘플링하고,\n","- model을 update한다."]},{"cell_type":"code","execution_count":26,"id":"e49c6175","metadata":{"id":"e49c6175","executionInfo":{"status":"ok","timestamp":1721703738325,"user_tz":-540,"elapsed":13,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["# sample a batch of _batch_size from replay buffers\n","# return numpy.ndarrays\n","def sample_batch(_batch_size):\n","    # Get indices of samples for replay buffers\n","    indices = np.random.choice(range(len(done_history)), size=_batch_size, replace=False)\n","\n","    state_sample = np.array([state_history[i].squeeze(0).numpy() for i in indices])\n","    state_next_sample = np.array([state_next_history[i].squeeze(0).numpy() for i in indices])\n","    rewards_sample = np.array([rewards_history[i] for i in indices], dtype=np.float32)\n","    action_sample = np.array([action_history[i] for i in indices])\n","\n","    # action mask is the mask for the valid actions at the '''next''' state\n","    action_mask_sample = np.array([action_mask_history[i] for i in indices])\n","    done_sample = np.array([float(done_history[i]) for i in indices])\n","\n","    return state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample"]},{"cell_type":"code","execution_count":27,"id":"b97e7ba6","metadata":{"id":"b97e7ba6","executionInfo":{"status":"ok","timestamp":1721703738325,"user_tz":-540,"elapsed":13,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["# Function to update the Q-network\n","def update_network():\n","    # sample a batch of ...\n","    state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample = \\\n","        sample_batch(batch_size)\n","\n","    # Convert numpy arrays to PyTorch tensors\n","    state_sample = torch.tensor(state_sample, dtype=torch.float32).to(device)\n","    state_next_sample = torch.tensor(state_next_sample, dtype=torch.float32).to(device)\n","    action_sample = torch.tensor(action_sample, dtype=torch.int64).to(device)\n","    action_mask_sample = torch.tensor(action_mask_sample, dtype=torch.int64).to(device)\n","    rewards_sample = torch.tensor(rewards_sample, dtype=torch.float32).to(device)\n","    done_sample = torch.tensor(done_sample, dtype=torch.float32).to(device)\n","\n","    # Compute the target Q-values for the states\n","    with torch.no_grad():\n","        future_rewards = model_target(state_next_sample)\n","        #future_rewards = future_rewards.cpu()\n","\n","        # compute the q-value for the next state and the action maximizing the q-value\n","        # note: the action should be valid (i.e., mask is set to 1)\n","        max_q_values = torch.max(\n","            future_rewards + action_mask_sample * 100., # trick to select a valid action\n","            dim=1).values.detach() - 100.\n","\n","        # compute the target q-value\n","        # if the step was final, max_q_values should not be added\n","        # we assume that the negative return of the opposite player is the return of next step\n","        # that is, G(t) = r(t+1) - g*r(t+2) + g^2*r(t+3) - g^3*r(t+4) + ...\n","        target_q_values = rewards_sample + gamma * max_q_values * (1. - done_sample)\n","\n","    # It's forward propagation! Compute the Q-values for the taken actions\n","    q_values = model(state_sample)\n","    #q_values = q_values.cpu()\n","    q_values_action = q_values.gather(dim=1, index=action_sample.unsqueeze(1)).squeeze(1)\n","\n","    # Compute the loss\n","    loss = loss_function(q_values_action, target_q_values)\n","\n","    # Perform the optimization step\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"]},{"cell_type":"markdown","id":"753d8339-d5db-403d-b97b-7cd0a901ad32","metadata":{"id":"753d8339-d5db-403d-b97b-7cd0a901ad32"},"source":["### Test A Single Tranining Step\n","\n","에피소드를 초기화 하고 첫 스텝 상태를 가져오기"]},{"cell_type":"code","execution_count":28,"id":"8e466783-3785-4cfc-a69b-64ed08ba919b","metadata":{"id":"8e466783-3785-4cfc-a69b-64ed08ba919b","executionInfo":{"status":"ok","timestamp":1721703738326,"user_tz":-540,"elapsed":13,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"67ef85a7-d1f4-4862-9de2-6b6c42b7b103"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 8, 1)"]},"metadata":{},"execution_count":28}],"source":["board, info = env.reset()\n","board.shape"]},{"cell_type":"code","execution_count":29,"id":"2063e219-a51a-4675-8adb-d30f83f2998c","metadata":{"id":"2063e219-a51a-4675-8adb-d30f83f2998c","executionInfo":{"status":"ok","timestamp":1721703739295,"user_tz":-540,"elapsed":979,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d7d78aff-4675-4dae-92af-54603da469d5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 8)"]},"metadata":{},"execution_count":29}],"source":["info['action_mask'].shape"]},{"cell_type":"code","execution_count":30,"id":"8825a287-2548-4729-90b5-b84e96016f0a","metadata":{"id":"8825a287-2548-4729-90b5-b84e96016f0a","executionInfo":{"status":"ok","timestamp":1721703739296,"user_tz":-540,"elapsed":88,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fef552e7-0abe-4920-999d-8d43db045124"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1., 1., 1., 1., 1., 1.]])"]},"metadata":{},"execution_count":30}],"source":["state = preprocess_state(board)\n","state[0] # the positions of stones of the agent player"]},{"cell_type":"code","execution_count":31,"id":"4f66147c-5e05-4e68-8dc2-37510d144a0d","metadata":{"id":"4f66147c-5e05-4e68-8dc2-37510d144a0d","executionInfo":{"status":"ok","timestamp":1721703739296,"user_tz":-540,"elapsed":82,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ec9869cf-5eba-4da2-b40b-da6c01314e1e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(64,)"]},"metadata":{},"execution_count":31}],"source":["action_mask = info['action_mask'].reshape((-1,))\n","action_mask.shape"]},{"cell_type":"code","execution_count":32,"id":"7f4d6730-4322-49e7-bee3-3911167db2ec","metadata":{"id":"7f4d6730-4322-49e7-bee3-3911167db2ec","executionInfo":{"status":"ok","timestamp":1721703739297,"user_tz":-540,"elapsed":77,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"2fc53949-d418-45e0-c8ea-1c2c845519fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["14\n"]}],"source":["action = get_greedy_epsilon(model, state, action_mask)\n","print(action)"]},{"cell_type":"markdown","id":"767b665e-7b98-44ac-92fa-fc6200e3680f","metadata":{"id":"767b665e-7b98-44ac-92fa-fc6200e3680f"},"source":["Get the next step from environment."]},{"cell_type":"code","execution_count":33,"id":"39acb122-1a45-4dc0-b47a-abd98499d4e0","metadata":{"id":"39acb122-1a45-4dc0-b47a-abd98499d4e0","executionInfo":{"status":"ok","timestamp":1721703739297,"user_tz":-540,"elapsed":73,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fd1aaa57-e2fb-48d8-e2fe-80d3c7c3388b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 8, 8])"]},"metadata":{},"execution_count":33}],"source":["board, reward, done, _, info = env.step((action // 8, action % 8))\n","state_next = preprocess_state(board)\n","action_mask = info['action_mask'].reshape((-1,))\n","state_next.shape"]},{"cell_type":"code","execution_count":34,"id":"4cb0af23-9019-43dd-bb5a-1070bd13de4a","metadata":{"id":"4cb0af23-9019-43dd-bb5a-1070bd13de4a","executionInfo":{"status":"ok","timestamp":1721703739304,"user_tz":-540,"elapsed":76,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"03b3d438-be1f-4f41-e03d-acdeb127cf3d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n","      dtype=int32)"]},"metadata":{},"execution_count":34}],"source":["action_mask"]},{"cell_type":"code","execution_count":35,"id":"5cf8fc02","metadata":{"id":"5cf8fc02","executionInfo":{"status":"ok","timestamp":1721703739305,"user_tz":-540,"elapsed":75,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"01662791-3240-463a-efab-e7d9264cfaa2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["-1"]},"metadata":{},"execution_count":35}],"source":["reward"]},{"cell_type":"markdown","id":"782dd0c9-d741-475f-9f40-051a1b099e59","metadata":{"id":"782dd0c9-d741-475f-9f40-051a1b099e59"},"source":["Put the two step into replay buffer."]},{"cell_type":"code","execution_count":36,"id":"ae3a990a-bb38-46f0-aa98-907c49dcb5cc","metadata":{"id":"ae3a990a-bb38-46f0-aa98-907c49dcb5cc","executionInfo":{"status":"ok","timestamp":1721703739305,"user_tz":-540,"elapsed":71,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["# Save actions and states in replay buffer\n","action_history.append(action)\n","action_mask_history.append(action_mask)\n","state_history.append(state)\n","state_next_history.append(state_next)\n","done_history.append(done)\n","rewards_history.append(reward)\n","\n","state = state_next"]},{"cell_type":"markdown","id":"598216f3-c186-4ffc-b2ce-085a58aeee0b","metadata":{"id":"598216f3-c186-4ffc-b2ce-085a58aeee0b"},"source":["Generate one more step."]},{"cell_type":"code","execution_count":37,"id":"79d71484-5e59-412c-b038-20bef74e37ed","metadata":{"id":"79d71484-5e59-412c-b038-20bef74e37ed","executionInfo":{"status":"ok","timestamp":1721703739305,"user_tz":-540,"elapsed":71,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"34a8acb1-ec19-46b0-ae8b-154abd9eaaa3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["16"]},"metadata":{},"execution_count":37}],"source":["# Get an action by epsilon-greedy policy\n","action = get_greedy_epsilon(model, state, action_mask)\n","action"]},{"cell_type":"code","execution_count":38,"id":"089aabc4-1f21-4ca0-96ff-40114777a391","metadata":{"id":"089aabc4-1f21-4ca0-96ff-40114777a391","executionInfo":{"status":"ok","timestamp":1721703739305,"user_tz":-540,"elapsed":65,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b55548ce-088b-48a5-ce39-edeec035d0d4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 8, 8])"]},"metadata":{},"execution_count":38}],"source":["board, reward, done, _, info = env.step((action // 8, action % 8))\n","state_next = preprocess_state(board)\n","action_mask = info['action_mask'].reshape((-1,))\n","state_next.shape"]},{"cell_type":"code","execution_count":39,"id":"a3c02582-ac0c-4975-ac0e-0bf37582e456","metadata":{"id":"a3c02582-ac0c-4975-ac0e-0bf37582e456","executionInfo":{"status":"ok","timestamp":1721703739305,"user_tz":-540,"elapsed":62,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["action_history.append(action)\n","action_mask_history.append(action_mask)\n","state_history.append(state)\n","state_next_history.append(state_next)\n","done_history.append(done)\n","rewards_history.append(reward)"]},{"cell_type":"markdown","id":"185b0e37-9544-49a6-a87a-2d612e424194","metadata":{"id":"185b0e37-9544-49a6-a87a-2d612e424194"},"source":["Test batch sampling."]},{"cell_type":"code","execution_count":40,"id":"2655ffa7-74e1-457d-b3dd-0cd834a034b4","metadata":{"id":"2655ffa7-74e1-457d-b3dd-0cd834a034b4","executionInfo":{"status":"ok","timestamp":1721703739306,"user_tz":-540,"elapsed":62,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample = \\\n","    sample_batch(2)"]},{"cell_type":"code","execution_count":41,"id":"6fd26d13","metadata":{"id":"6fd26d13","executionInfo":{"status":"ok","timestamp":1721703739306,"user_tz":-540,"elapsed":61,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"cf6bcfcf-8ebf-4da4-8e8d-a1fcd39a1c4e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2, 3, 8, 8)"]},"metadata":{},"execution_count":41}],"source":["state_sample.shape"]},{"cell_type":"code","execution_count":42,"id":"f08d576b","metadata":{"id":"f08d576b","executionInfo":{"status":"ok","timestamp":1721703739306,"user_tz":-540,"elapsed":58,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a47fcf22-d008-4f6e-f2ad-08bb4471cb37"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2, 3, 8, 8)"]},"metadata":{},"execution_count":42}],"source":["state_next_sample.shape"]},{"cell_type":"code","execution_count":43,"id":"ede572e9","metadata":{"id":"ede572e9","executionInfo":{"status":"ok","timestamp":1721703739306,"user_tz":-540,"elapsed":55,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4cbb7631-1898-4efe-8a67-efda0576f4b9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2,)"]},"metadata":{},"execution_count":43}],"source":["rewards_sample.shape"]},{"cell_type":"code","execution_count":44,"id":"4739fee9","metadata":{"id":"4739fee9","executionInfo":{"status":"ok","timestamp":1721703739306,"user_tz":-540,"elapsed":51,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e64d80c8-e68a-4743-df76-306ff4aa34f7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2,)"]},"metadata":{},"execution_count":44}],"source":["action_sample.shape"]},{"cell_type":"code","execution_count":45,"id":"e8ab78fe","metadata":{"id":"e8ab78fe","executionInfo":{"status":"ok","timestamp":1721703739307,"user_tz":-540,"elapsed":49,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"91bebc70-06cc-4784-8ccc-4b668c9f06ac"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2, 64)"]},"metadata":{},"execution_count":45}],"source":["action_mask_sample.shape"]},{"cell_type":"code","execution_count":46,"id":"34c6a22e","metadata":{"id":"34c6a22e","executionInfo":{"status":"ok","timestamp":1721703739307,"user_tz":-540,"elapsed":46,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["# Convert numpy arrays to PyTorch tensors\n","state_sample = torch.tensor(state_sample, dtype=torch.float32)\n","state_next_sample = torch.tensor(state_next_sample, dtype=torch.float32)\n","action_sample = torch.tensor(action_sample, dtype=torch.int64)\n","action_mask_sample = torch.tensor(action_mask_sample, dtype=torch.int64)\n","rewards_sample = torch.tensor(rewards_sample, dtype=torch.float32)\n","done_sample = torch.tensor(done_sample, dtype=torch.float32)"]},{"cell_type":"markdown","id":"7ac52d51-3d70-4e18-9b60-5124463b6777","metadata":{"id":"7ac52d51-3d70-4e18-9b60-5124463b6777"},"source":["Test model prediction with the batch."]},{"cell_type":"code","execution_count":47,"id":"c41c20cd-d1fd-4416-a3cf-13b35e2424b2","metadata":{"id":"c41c20cd-d1fd-4416-a3cf-13b35e2424b2","executionInfo":{"status":"ok","timestamp":1721703739307,"user_tz":-540,"elapsed":45,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9815f7f8-6ff6-4004-c723-1fa7b483f165"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2, 64])"]},"metadata":{},"execution_count":47}],"source":["with torch.no_grad():\n","    future_rewards = model_target(state_next_sample.to(device))\n","\n","future_rewards.shape"]},{"cell_type":"code","execution_count":48,"id":"d4833a5d","metadata":{"id":"d4833a5d","executionInfo":{"status":"ok","timestamp":1721703739307,"user_tz":-540,"elapsed":42,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"9926221f-85d1-4567-9d95-9d3ff6555482"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([16, 14])"]},"metadata":{},"execution_count":48}],"source":["action_sample"]},{"cell_type":"code","execution_count":49,"id":"add0ecb5","metadata":{"id":"add0ecb5","executionInfo":{"status":"ok","timestamp":1721703739308,"user_tz":-540,"elapsed":39,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"26e46c6e-c23b-4e38-bd2f-6697fdc79dc4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.return_types.max(\n","values=tensor([0.0737, 0.0940]),\n","indices=tensor([61, 13]))"]},"metadata":{},"execution_count":49}],"source":["torch.max(future_rewards, dim=1)"]},{"cell_type":"code","execution_count":50,"id":"7cb07686","metadata":{"id":"7cb07686","executionInfo":{"status":"ok","timestamp":1721703739308,"user_tz":-540,"elapsed":36,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a286e5f8-4164-405f-cb50-8eac422d1f16"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.return_types.max(\n","values=tensor([100.0737, 100.0940]),\n","indices=tensor([61, 13]))"]},"metadata":{},"execution_count":50}],"source":["torch.max(\n","        future_rewards + action_mask_sample.to(device) * 100.,\n","        dim=1)"]},{"cell_type":"code","execution_count":51,"id":"ca434acf","metadata":{"id":"ca434acf","executionInfo":{"status":"ok","timestamp":1721703739308,"user_tz":-540,"elapsed":33,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"421c3c9a-c725-4f2f-f3e6-362dfa06e2eb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0.0737, 0.0940])"]},"metadata":{},"execution_count":51}],"source":["torch.max(\n","        future_rewards + action_mask_sample.to(device) * 100.,\n","        dim=1).values.detach() - 100."]},{"cell_type":"code","execution_count":52,"id":"909f6afb","metadata":{"id":"909f6afb","executionInfo":{"status":"ok","timestamp":1721703739308,"user_tz":-540,"elapsed":30,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"39180f94-2d08-4aec-ed7d-032ffdcd5c83"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.],\n","        [0.]])"]},"metadata":{},"execution_count":52}],"source":["done_sample.unsqueeze(1)"]},{"cell_type":"code","execution_count":53,"id":"8a534020","metadata":{"id":"8a534020","executionInfo":{"status":"ok","timestamp":1721703739308,"user_tz":-540,"elapsed":27,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c77aaa6f-13ab-47f8-dcdd-59a257416e10"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2])"]},"metadata":{},"execution_count":53}],"source":["with torch.no_grad():\n","    future_rewards = model_target(state_next_sample.to(device))\n","\n","    # compute the q-value for the next state and the action maximizing the q-value\n","    # note: the action should be valid (i.e., mask is set to 1)\n","    max_q_values = torch.max(\n","        future_rewards + action_mask_sample.to(device) * 100., # trick to select a valid action\n","        dim=1).values.detach() - 100.\n","\n","    # compute the target q-value\n","    # if the step was final, max_q_values should not be added\n","    target_q_values = rewards_sample.to(device) +  max_q_values * (1. - done_sample.to(device))\n","\n","target_q_values.shape"]},{"cell_type":"code","execution_count":54,"id":"de7d19a9","metadata":{"id":"de7d19a9","executionInfo":{"status":"ok","timestamp":1721703739308,"user_tz":-540,"elapsed":24,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"35e69f77-87b5-405c-d478-791c3d239d26"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2])"]},"metadata":{},"execution_count":54}],"source":["# It's forward propagation! Compute the Q-values for the taken actions\n","q_values = model(state_sample.to(device))\n","q_values_action = q_values.gather(dim=1, index=action_sample.to(device).unsqueeze(1)).squeeze(1)\n","q_values_action.shape"]},{"cell_type":"code","execution_count":55,"id":"3a1343c8","metadata":{"id":"3a1343c8","executionInfo":{"status":"ok","timestamp":1721703739890,"user_tz":-540,"elapsed":603,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["# Compute the loss\n","loss = loss_function(q_values_action, target_q_values)"]},{"cell_type":"code","execution_count":56,"id":"a4eb9ffb","metadata":{"id":"a4eb9ffb","executionInfo":{"status":"ok","timestamp":1721703739891,"user_tz":-540,"elapsed":603,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"outputs":[],"source":["# Perform the optimization step\n","optimizer.zero_grad()\n","loss.backward()\n","optimizer.step()"]},{"cell_type":"markdown","id":"90ed4dcc-f24e-4990-879a-c2f6af51a063","metadata":{"id":"90ed4dcc-f24e-4990-879a-c2f6af51a063"},"source":["# Run DQN Tranining"]},{"cell_type":"code","execution_count":57,"id":"43c0a265","metadata":{"id":"43c0a265","executionInfo":{"status":"ok","timestamp":1721705915108,"user_tz":-540,"elapsed":2175223,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b8351b39-3088-43f6-9d66-da9aaa8103a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 10, Frame count: 529, Running reward: -33.5\n","Episode: 20, Frame count: 1106, Running reward: -36.6\n","Episode: 30, Frame count: 1692, Running reward: -37.733333333333334\n","Episode: 40, Frame count: 2261, Running reward: -37.775\n","Episode: 50, Frame count: 2826, Running reward: -37.76\n","Episode: 60, Frame count: 3384, Running reward: -37.56666666666667\n","Episode: 70, Frame count: 3946, Running reward: -37.48571428571429\n","Episode: 80, Frame count: 4528, Running reward: -37.85\n","Episode: 90, Frame count: 5092, Running reward: -37.86666666666667\n","Episode: 100, Frame count: 5673, Running reward: -38.05\n","Episode: 110, Frame count: 6251, Running reward: -38.68\n","Episode: 120, Frame count: 6820, Running reward: -38.46\n","Episode: 130, Frame count: 7379, Running reward: -38.19\n","Episode: 140, Frame count: 7937, Running reward: -38.1\n","Episode: 150, Frame count: 8519, Running reward: -38.29\n","Episode: 160, Frame count: 9069, Running reward: -38.23\n","Episode: 170, Frame count: 9608, Running reward: -37.96\n","Episode: 180, Frame count: 10161, Running reward: -37.57\n","Episode: 190, Frame count: 10723, Running reward: -37.45\n","Episode: 200, Frame count: 11253, Running reward: -36.88\n","Episode: 210, Frame count: 11785, Running reward: -36.26\n","Episode: 220, Frame count: 12349, Running reward: -36.27\n","Episode: 230, Frame count: 12873, Running reward: -35.84\n","Episode: 240, Frame count: 13403, Running reward: -35.48\n","Episode: 250, Frame count: 13927, Running reward: -34.92\n","Episode: 260, Frame count: 14451, Running reward: -34.6\n","Episode: 270, Frame count: 14992, Running reward: -34.68\n","Episode: 280, Frame count: 15529, Running reward: -34.56\n","Episode: 290, Frame count: 16070, Running reward: -34.41\n","Episode: 300, Frame count: 16613, Running reward: -34.56\n","Episode: 310, Frame count: 17151, Running reward: -34.68\n","Episode: 320, Frame count: 17664, Running reward: -34.09\n","Episode: 330, Frame count: 18218, Running reward: -34.43\n","Episode: 340, Frame count: 18738, Running reward: -34.33\n","Episode: 350, Frame count: 19283, Running reward: -34.48\n","Episode: 360, Frame count: 19786, Running reward: -34.23\n","Episode: 370, Frame count: 20285, Running reward: -33.71\n","Episode: 380, Frame count: 20764, Running reward: -32.99\n","Episode: 390, Frame count: 21301, Running reward: -32.89\n","Episode: 400, Frame count: 21826, Running reward: -32.59\n","Episode: 410, Frame count: 22349, Running reward: -32.36\n","Episode: 420, Frame count: 22843, Running reward: -32.15\n","Episode: 430, Frame count: 23386, Running reward: -32.06\n","Episode: 440, Frame count: 23897, Running reward: -32.03\n","Episode: 450, Frame count: 24394, Running reward: -31.51\n","Episode: 460, Frame count: 24896, Running reward: -31.5\n","Episode: 470, Frame count: 25412, Running reward: -31.77\n","Episode: 480, Frame count: 25958, Running reward: -32.46\n","Episode: 490, Frame count: 26430, Running reward: -31.77\n","Episode: 500, Frame count: 26931, Running reward: -31.53\n","Episode: 510, Frame count: 27425, Running reward: -31.22\n","Episode: 520, Frame count: 27901, Running reward: -31.06\n","Episode: 530, Frame count: 28387, Running reward: -30.37\n","Episode: 540, Frame count: 28831, Running reward: -29.62\n","Episode: 550, Frame count: 29356, Running reward: -29.92\n","Episode: 560, Frame count: 29816, Running reward: -29.5\n","Episode: 570, Frame count: 30342, Running reward: -29.56\n","Episode: 580, Frame count: 30864, Running reward: -29.32\n","Episode: 590, Frame count: 31303, Running reward: -28.99\n","Episode: 600, Frame count: 31803, Running reward: -29.02\n","Episode: 610, Frame count: 32262, Running reward: -28.67\n","Episode: 620, Frame count: 32759, Running reward: -28.88\n","Episode: 630, Frame count: 33172, Running reward: -28.15\n","Episode: 640, Frame count: 33620, Running reward: -28.19\n","Episode: 650, Frame count: 34059, Running reward: -27.25\n","Episode: 660, Frame count: 34552, Running reward: -27.6\n","Episode: 670, Frame count: 34994, Running reward: -26.7\n","Episode: 680, Frame count: 35477, Running reward: -26.35\n","Episode: 690, Frame count: 35935, Running reward: -26.54\n","Episode: 700, Frame count: 36378, Running reward: -25.93\n","Episode: 710, Frame count: 36847, Running reward: -26.03\n","Episode: 720, Frame count: 37305, Running reward: -25.64\n","Episode: 730, Frame count: 37780, Running reward: -26.26\n","Episode: 740, Frame count: 38231, Running reward: -26.31\n","Episode: 750, Frame count: 38669, Running reward: -26.3\n","Episode: 760, Frame count: 39146, Running reward: -26.16\n","Episode: 770, Frame count: 39637, Running reward: -26.67\n","Episode: 780, Frame count: 40092, Running reward: -26.33\n","Episode: 790, Frame count: 40560, Running reward: -26.43\n","Episode: 800, Frame count: 41064, Running reward: -27.06\n","Episode: 810, Frame count: 41481, Running reward: -26.54\n","Episode: 820, Frame count: 41939, Running reward: -26.5\n","Episode: 830, Frame count: 42389, Running reward: -26.25\n","Episode: 840, Frame count: 42811, Running reward: -25.94\n","Episode: 850, Frame count: 43286, Running reward: -26.31\n","Episode: 860, Frame count: 43730, Running reward: -25.96\n","Episode: 870, Frame count: 44155, Running reward: -25.3\n","Episode: 880, Frame count: 44622, Running reward: -25.4\n","Episode: 890, Frame count: 45088, Running reward: -25.36\n","Episode: 900, Frame count: 45517, Running reward: -24.59\n","Episode: 910, Frame count: 45960, Running reward: -24.85\n","Episode: 920, Frame count: 46398, Running reward: -24.65\n","Episode: 930, Frame count: 46840, Running reward: -24.57\n","Episode: 940, Frame count: 47266, Running reward: -24.59\n","Episode: 950, Frame count: 47707, Running reward: -24.25\n","Episode: 960, Frame count: 48152, Running reward: -24.24\n","Episode: 970, Frame count: 48600, Running reward: -24.47\n","Episode: 980, Frame count: 49062, Running reward: -24.42\n","Episode: 990, Frame count: 49451, Running reward: -23.67\n","Episode: 1000, Frame count: 49862, Running reward: -23.51\n","Episode: 1010, Frame count: 50303, Running reward: -23.49\n","Episode: 1020, Frame count: 50694, Running reward: -23.02\n","Episode: 1030, Frame count: 51156, Running reward: -23.22\n","Episode: 1040, Frame count: 51575, Running reward: -23.15\n","Episode: 1050, Frame count: 52014, Running reward: -23.15\n","Episode: 1060, Frame count: 52407, Running reward: -22.63\n","Episode: 1070, Frame count: 52794, Running reward: -22.0\n","Episode: 1080, Frame count: 53178, Running reward: -21.22\n","Episode: 1090, Frame count: 53610, Running reward: -21.63\n","Episode: 1100, Frame count: 54050, Running reward: -21.9\n","Episode: 1110, Frame count: 54450, Running reward: -21.49\n","Episode: 1120, Frame count: 54878, Running reward: -21.86\n","Episode: 1130, Frame count: 55303, Running reward: -21.49\n","Episode: 1140, Frame count: 55694, Running reward: -21.21\n","Episode: 1150, Frame count: 56133, Running reward: -21.21\n","Episode: 1160, Frame count: 56495, Running reward: -20.9\n","Episode: 1170, Frame count: 56855, Running reward: -20.63\n","Episode: 1180, Frame count: 57227, Running reward: -20.51\n","Episode: 1190, Frame count: 57648, Running reward: -20.4\n","Episode: 1200, Frame count: 58034, Running reward: -19.86\n","Episode: 1210, Frame count: 58442, Running reward: -19.94\n","Episode: 1220, Frame count: 58847, Running reward: -19.73\n","Episode: 1230, Frame count: 59225, Running reward: -19.26\n","Episode: 1240, Frame count: 59625, Running reward: -19.35\n","Episode: 1250, Frame count: 60026, Running reward: -18.95\n","Episode: 1260, Frame count: 60462, Running reward: -19.71\n","Episode: 1270, Frame count: 60868, Running reward: -20.17\n","Episode: 1280, Frame count: 61275, Running reward: -20.52\n","Episode: 1290, Frame count: 61707, Running reward: -20.63\n","Episode: 1300, Frame count: 62084, Running reward: -20.54\n","Episode: 1310, Frame count: 62453, Running reward: -20.15\n","Episode: 1320, Frame count: 62830, Running reward: -19.85\n","Episode: 1330, Frame count: 63261, Running reward: -20.38\n","Episode: 1340, Frame count: 63694, Running reward: -20.71\n","Episode: 1350, Frame count: 64118, Running reward: -20.94\n","Episode: 1360, Frame count: 64465, Running reward: -20.03\n","Episode: 1370, Frame count: 64854, Running reward: -19.88\n","Episode: 1380, Frame count: 65243, Running reward: -19.7\n","Episode: 1390, Frame count: 65629, Running reward: -19.24\n","Episode: 1400, Frame count: 66018, Running reward: -19.36\n","Episode: 1410, Frame count: 66429, Running reward: -19.78\n","Episode: 1420, Frame count: 66781, Running reward: -19.53\n","Episode: 1430, Frame count: 67148, Running reward: -18.89\n","Episode: 1440, Frame count: 67505, Running reward: -18.13\n","Episode: 1450, Frame count: 67873, Running reward: -17.57\n","Episode: 1460, Frame count: 68268, Running reward: -18.05\n","Episode: 1470, Frame count: 68648, Running reward: -17.94\n","Episode: 1480, Frame count: 68994, Running reward: -17.51\n","Episode: 1490, Frame count: 69350, Running reward: -17.21\n","Episode: 1500, Frame count: 69746, Running reward: -17.3\n","Episode: 1510, Frame count: 70174, Running reward: -17.47\n","Episode: 1520, Frame count: 70528, Running reward: -17.49\n","Episode: 1530, Frame count: 70898, Running reward: -17.52\n","Episode: 1540, Frame count: 71281, Running reward: -17.78\n","Episode: 1550, Frame count: 71633, Running reward: -17.62\n","Episode: 1560, Frame count: 71962, Running reward: -16.96\n","Episode: 1570, Frame count: 72329, Running reward: -16.83\n","Episode: 1580, Frame count: 72669, Running reward: -16.77\n","Episode: 1590, Frame count: 73016, Running reward: -16.68\n","Episode: 1600, Frame count: 73407, Running reward: -16.61\n","Episode: 1610, Frame count: 73784, Running reward: -16.1\n","Episode: 1620, Frame count: 74126, Running reward: -15.98\n","Episode: 1630, Frame count: 74464, Running reward: -15.66\n","Episode: 1640, Frame count: 74813, Running reward: -15.32\n","Episode: 1650, Frame count: 75163, Running reward: -15.3\n","Episode: 1660, Frame count: 75516, Running reward: -15.54\n","Episode: 1670, Frame count: 75908, Running reward: -15.79\n","Episode: 1680, Frame count: 76272, Running reward: -16.03\n","Episode: 1690, Frame count: 76647, Running reward: -16.31\n","Episode: 1700, Frame count: 76966, Running reward: -15.59\n","Episode: 1710, Frame count: 77324, Running reward: -15.4\n","Episode: 1720, Frame count: 77664, Running reward: -15.38\n","Episode: 1730, Frame count: 78024, Running reward: -15.6\n","Episode: 1740, Frame count: 78349, Running reward: -15.36\n","Episode: 1750, Frame count: 78688, Running reward: -15.25\n","Episode: 1760, Frame count: 79042, Running reward: -15.26\n","Episode: 1770, Frame count: 79386, Running reward: -14.78\n","Episode: 1780, Frame count: 79762, Running reward: -14.9\n","Episode: 1790, Frame count: 80091, Running reward: -14.44\n","Episode: 1800, Frame count: 80423, Running reward: -14.57\n","Episode: 1810, Frame count: 80770, Running reward: -14.46\n","Episode: 1820, Frame count: 81127, Running reward: -14.63\n","Episode: 1830, Frame count: 81437, Running reward: -14.13\n","Episode: 1840, Frame count: 81777, Running reward: -14.28\n","Episode: 1850, Frame count: 82101, Running reward: -14.13\n","Episode: 1860, Frame count: 82403, Running reward: -13.61\n","Episode: 1870, Frame count: 82747, Running reward: -13.61\n","Episode: 1880, Frame count: 83089, Running reward: -13.27\n","Episode: 1890, Frame count: 83392, Running reward: -13.01\n","Episode: 1900, Frame count: 83746, Running reward: -13.23\n","Episode: 1910, Frame count: 84049, Running reward: -12.79\n","Episode: 1920, Frame count: 84422, Running reward: -12.95\n","Episode: 1930, Frame count: 84726, Running reward: -12.89\n","Episode: 1940, Frame count: 85009, Running reward: -12.32\n","Episode: 1950, Frame count: 85362, Running reward: -12.61\n","Episode: 1960, Frame count: 85705, Running reward: -13.02\n","Episode: 1970, Frame count: 86022, Running reward: -12.75\n","Episode: 1980, Frame count: 86347, Running reward: -12.58\n","Episode: 1990, Frame count: 86637, Running reward: -12.45\n","Episode: 2000, Frame count: 87020, Running reward: -12.74\n","Episode: 2010, Frame count: 87314, Running reward: -12.65\n","Episode: 2020, Frame count: 87628, Running reward: -12.06\n","Episode: 2030, Frame count: 87960, Running reward: -12.34\n","Episode: 2040, Frame count: 88324, Running reward: -13.15\n","Episode: 2050, Frame count: 88617, Running reward: -12.55\n","Episode: 2060, Frame count: 88930, Running reward: -12.25\n","Episode: 2070, Frame count: 89260, Running reward: -12.38\n","Episode: 2080, Frame count: 89560, Running reward: -12.13\n","Episode: 2090, Frame count: 89845, Running reward: -12.08\n","Episode: 2100, Frame count: 90154, Running reward: -11.34\n","Episode: 2110, Frame count: 90474, Running reward: -11.6\n","Episode: 2120, Frame count: 90775, Running reward: -11.47\n","Episode: 2130, Frame count: 91060, Running reward: -11.0\n","Episode: 2140, Frame count: 91375, Running reward: -10.51\n","Episode: 2150, Frame count: 91705, Running reward: -10.88\n","Episode: 2160, Frame count: 92025, Running reward: -10.95\n","Episode: 2170, Frame count: 92303, Running reward: -10.43\n","Episode: 2180, Frame count: 92589, Running reward: -10.29\n","Episode: 2190, Frame count: 92909, Running reward: -10.64\n","Episode: 2200, Frame count: 93214, Running reward: -10.6\n","Episode: 2210, Frame count: 93511, Running reward: -10.37\n","Episode: 2220, Frame count: 93781, Running reward: -10.06\n","Episode: 2230, Frame count: 94057, Running reward: -9.97\n","Episode: 2240, Frame count: 94330, Running reward: -9.55\n","Episode: 2250, Frame count: 94650, Running reward: -9.45\n","Episode: 2260, Frame count: 94960, Running reward: -9.35\n","Episode: 2270, Frame count: 95266, Running reward: -9.63\n","Episode: 2280, Frame count: 95607, Running reward: -10.18\n","Episode: 2290, Frame count: 95878, Running reward: -9.69\n","Episode: 2300, Frame count: 96155, Running reward: -9.41\n","Episode: 2310, Frame count: 96473, Running reward: -9.62\n","Episode: 2320, Frame count: 96758, Running reward: -9.77\n","Episode: 2330, Frame count: 97043, Running reward: -9.86\n","Episode: 2340, Frame count: 97386, Running reward: -10.56\n","Episode: 2350, Frame count: 97689, Running reward: -10.39\n","Episode: 2360, Frame count: 98005, Running reward: -10.45\n","Episode: 2370, Frame count: 98323, Running reward: -10.57\n","Episode: 2380, Frame count: 98609, Running reward: -10.02\n","Episode: 2390, Frame count: 98880, Running reward: -10.02\n","Episode: 2400, Frame count: 99178, Running reward: -10.23\n","Episode: 2410, Frame count: 99503, Running reward: -10.3\n","Episode: 2420, Frame count: 99785, Running reward: -10.27\n","Episode: 2430, Frame count: 100089, Running reward: -10.46\n","Episode: 2440, Frame count: 100417, Running reward: -10.31\n","Episode: 2450, Frame count: 100685, Running reward: -9.96\n","Episode: 2460, Frame count: 100963, Running reward: -9.58\n","Episode: 2470, Frame count: 101267, Running reward: -9.44\n","Episode: 2480, Frame count: 101552, Running reward: -9.43\n","Episode: 2490, Frame count: 101867, Running reward: -9.87\n","Episode: 2500, Frame count: 102130, Running reward: -9.52\n","Episode: 2510, Frame count: 102423, Running reward: -9.2\n","Episode: 2520, Frame count: 102717, Running reward: -9.32\n","Episode: 2530, Frame count: 102974, Running reward: -8.85\n","Episode: 2540, Frame count: 103278, Running reward: -8.61\n","Episode: 2550, Frame count: 103577, Running reward: -8.92\n","Episode: 2560, Frame count: 103872, Running reward: -9.09\n","Episode: 2570, Frame count: 104163, Running reward: -8.96\n","Episode: 2580, Frame count: 104419, Running reward: -8.67\n","Episode: 2590, Frame count: 104712, Running reward: -8.45\n","Episode: 2600, Frame count: 105042, Running reward: -9.12\n","Episode: 2610, Frame count: 105371, Running reward: -9.48\n","Episode: 2620, Frame count: 105646, Running reward: -9.29\n","Episode: 2630, Frame count: 105920, Running reward: -9.46\n","Episode: 2640, Frame count: 106180, Running reward: -9.02\n","Episode: 2650, Frame count: 106454, Running reward: -8.77\n","Episode: 2660, Frame count: 106728, Running reward: -8.56\n","Episode: 2670, Frame count: 106995, Running reward: -8.32\n","Episode: 2680, Frame count: 107281, Running reward: -8.62\n","Episode: 2690, Frame count: 107564, Running reward: -8.52\n","Episode: 2700, Frame count: 107819, Running reward: -7.77\n","Episode: 2710, Frame count: 108098, Running reward: -7.27\n","Episode: 2720, Frame count: 108326, Running reward: -6.8\n","Episode: 2730, Frame count: 108604, Running reward: -6.84\n","Episode: 2740, Frame count: 108845, Running reward: -6.65\n","Episode: 2750, Frame count: 109104, Running reward: -6.5\n","Episode: 2760, Frame count: 109358, Running reward: -6.3\n","Episode: 2770, Frame count: 109659, Running reward: -6.64\n","Episode: 2780, Frame count: 109937, Running reward: -6.56\n","Episode: 2790, Frame count: 110212, Running reward: -6.48\n","Episode: 2800, Frame count: 110471, Running reward: -6.52\n","Episode: 2810, Frame count: 110788, Running reward: -6.9\n","Episode: 2820, Frame count: 111102, Running reward: -7.76\n","Episode: 2830, Frame count: 111404, Running reward: -8.0\n","Episode: 2840, Frame count: 111686, Running reward: -8.41\n","Episode: 2850, Frame count: 111915, Running reward: -8.11\n","Episode: 2860, Frame count: 112228, Running reward: -8.7\n","Episode: 2870, Frame count: 112472, Running reward: -8.13\n","Episode: 2880, Frame count: 112702, Running reward: -7.65\n","Episode: 2890, Frame count: 112991, Running reward: -7.79\n","Episode: 2900, Frame count: 113248, Running reward: -7.77\n","Episode: 2910, Frame count: 113523, Running reward: -7.35\n","Episode: 2920, Frame count: 113784, Running reward: -6.82\n","Episode: 2930, Frame count: 114052, Running reward: -6.48\n","Episode: 2940, Frame count: 114289, Running reward: -6.03\n","Episode: 2950, Frame count: 114503, Running reward: -5.88\n","Episode: 2960, Frame count: 114739, Running reward: -5.11\n","Episode: 2970, Frame count: 114995, Running reward: -5.23\n","Episode: 2980, Frame count: 115264, Running reward: -5.62\n","Episode: 2990, Frame count: 115529, Running reward: -5.38\n","Episode: 3000, Frame count: 115787, Running reward: -5.39\n","Episode: 3010, Frame count: 116071, Running reward: -5.48\n","Episode: 3020, Frame count: 116329, Running reward: -5.45\n","Episode: 3030, Frame count: 116592, Running reward: -5.4\n","Episode: 3040, Frame count: 116871, Running reward: -5.82\n","Episode: 3050, Frame count: 117140, Running reward: -6.37\n","Episode: 3060, Frame count: 117374, Running reward: -6.35\n","Episode: 3070, Frame count: 117646, Running reward: -6.51\n","Episode: 3080, Frame count: 117878, Running reward: -6.14\n","Episode: 3090, Frame count: 118129, Running reward: -6.0\n","Episode: 3100, Frame count: 118417, Running reward: -6.3\n","Episode: 3110, Frame count: 118678, Running reward: -6.07\n","Episode: 3120, Frame count: 118959, Running reward: -6.3\n","Episode: 3130, Frame count: 119221, Running reward: -6.29\n","Episode: 3140, Frame count: 119489, Running reward: -6.18\n","Episode: 3150, Frame count: 119760, Running reward: -6.2\n","Episode: 3160, Frame count: 119991, Running reward: -6.17\n","Episode: 3170, Frame count: 120231, Running reward: -5.85\n","Episode: 3180, Frame count: 120479, Running reward: -6.01\n","Episode: 3190, Frame count: 120758, Running reward: -6.29\n","Episode: 3200, Frame count: 120995, Running reward: -5.78\n","Episode: 3210, Frame count: 121287, Running reward: -6.09\n","Episode: 3220, Frame count: 121525, Running reward: -5.66\n","Episode: 3230, Frame count: 121767, Running reward: -5.46\n","Episode: 3240, Frame count: 122007, Running reward: -5.18\n","Episode: 3250, Frame count: 122226, Running reward: -4.66\n","Episode: 3260, Frame count: 122471, Running reward: -4.8\n","Episode: 3270, Frame count: 122739, Running reward: -5.08\n","Episode: 3280, Frame count: 122989, Running reward: -5.1\n","Episode: 3290, Frame count: 123231, Running reward: -4.73\n","Episode: 3300, Frame count: 123468, Running reward: -4.73\n","Episode: 3310, Frame count: 123690, Running reward: -4.03\n","Episode: 3320, Frame count: 123941, Running reward: -4.16\n","Episode: 3330, Frame count: 124178, Running reward: -4.11\n","Episode: 3340, Frame count: 124443, Running reward: -4.36\n","Episode: 3350, Frame count: 124696, Running reward: -4.7\n","Episode: 3360, Frame count: 124954, Running reward: -4.83\n","Episode: 3370, Frame count: 125197, Running reward: -4.58\n","Episode: 3380, Frame count: 125428, Running reward: -4.39\n","Episode: 3390, Frame count: 125665, Running reward: -4.34\n","Episode: 3400, Frame count: 125965, Running reward: -4.97\n","Episode: 3410, Frame count: 126212, Running reward: -5.22\n","Episode: 3420, Frame count: 126453, Running reward: -5.12\n","Episode: 3430, Frame count: 126687, Running reward: -5.09\n","Episode: 3440, Frame count: 126911, Running reward: -4.68\n","Episode: 3450, Frame count: 127164, Running reward: -4.68\n","Episode: 3460, Frame count: 127371, Running reward: -4.17\n","Episode: 3470, Frame count: 127631, Running reward: -4.34\n","Episode: 3480, Frame count: 127830, Running reward: -4.02\n","Episode: 3490, Frame count: 128053, Running reward: -3.88\n","Episode: 3500, Frame count: 128323, Running reward: -3.58\n","Episode: 3510, Frame count: 128565, Running reward: -3.53\n","Episode: 3520, Frame count: 128797, Running reward: -3.44\n","Episode: 3530, Frame count: 129039, Running reward: -3.52\n","Episode: 3540, Frame count: 129274, Running reward: -3.63\n","Episode: 3550, Frame count: 129536, Running reward: -3.72\n","Episode: 3560, Frame count: 129757, Running reward: -3.86\n","Episode: 3570, Frame count: 130006, Running reward: -3.75\n","Episode: 3580, Frame count: 130223, Running reward: -3.93\n","Episode: 3590, Frame count: 130472, Running reward: -4.19\n","Episode: 3600, Frame count: 130725, Running reward: -4.02\n","Episode: 3610, Frame count: 130971, Running reward: -4.06\n","Episode: 3620, Frame count: 131181, Running reward: -3.84\n","Episode: 3630, Frame count: 131410, Running reward: -3.71\n","Episode: 3640, Frame count: 131664, Running reward: -3.9\n","Episode: 3650, Frame count: 131887, Running reward: -3.51\n","Episode: 3660, Frame count: 132123, Running reward: -3.66\n","Episode: 3670, Frame count: 132397, Running reward: -3.91\n","Episode: 3680, Frame count: 132644, Running reward: -4.21\n","Episode: 3690, Frame count: 132906, Running reward: -4.34\n","Episode: 3700, Frame count: 133157, Running reward: -4.32\n","Episode: 3710, Frame count: 133386, Running reward: -4.15\n","Episode: 3720, Frame count: 133623, Running reward: -4.42\n","Episode: 3730, Frame count: 133863, Running reward: -4.53\n","Episode: 3740, Frame count: 134082, Running reward: -4.18\n","Episode: 3750, Frame count: 134309, Running reward: -4.22\n","Episode: 3760, Frame count: 134561, Running reward: -4.38\n","Episode: 3770, Frame count: 134808, Running reward: -4.11\n","Episode: 3780, Frame count: 135050, Running reward: -4.06\n","Episode: 3790, Frame count: 135300, Running reward: -3.94\n","Episode: 3800, Frame count: 135547, Running reward: -3.9\n","Episode: 3810, Frame count: 135785, Running reward: -3.99\n","Episode: 3820, Frame count: 136003, Running reward: -3.8\n","Episode: 3830, Frame count: 136251, Running reward: -3.88\n","Episode: 3840, Frame count: 136478, Running reward: -3.96\n","Episode: 3850, Frame count: 136686, Running reward: -3.77\n","Episode: 3860, Frame count: 136914, Running reward: -3.53\n","Episode: 3870, Frame count: 137149, Running reward: -3.41\n","Episode: 3880, Frame count: 137375, Running reward: -3.25\n","Episode: 3890, Frame count: 137591, Running reward: -2.91\n","Episode: 3900, Frame count: 137809, Running reward: -2.62\n","Episode: 3910, Frame count: 138054, Running reward: -2.69\n","Episode: 3920, Frame count: 138284, Running reward: -2.81\n","Episode: 3930, Frame count: 138510, Running reward: -2.59\n","Episode: 3940, Frame count: 138717, Running reward: -2.39\n","Episode: 3950, Frame count: 138930, Running reward: -2.44\n","Episode: 3960, Frame count: 139163, Running reward: -2.49\n","Episode: 3970, Frame count: 139397, Running reward: -2.48\n","Episode: 3980, Frame count: 139607, Running reward: -2.32\n","Episode: 3990, Frame count: 139826, Running reward: -2.35\n","Episode: 4000, Frame count: 140052, Running reward: -2.43\n","Episode: 4010, Frame count: 140263, Running reward: -2.09\n","Episode: 4020, Frame count: 140469, Running reward: -1.85\n","Episode: 4030, Frame count: 140702, Running reward: -1.92\n","Episode: 4040, Frame count: 140908, Running reward: -1.91\n","Episode: 4050, Frame count: 141137, Running reward: -2.07\n","Episode: 4060, Frame count: 141354, Running reward: -1.91\n","Episode: 4070, Frame count: 141596, Running reward: -1.99\n","Episode: 4080, Frame count: 141831, Running reward: -2.24\n","Episode: 4090, Frame count: 142070, Running reward: -2.44\n","Episode: 4100, Frame count: 142304, Running reward: -2.52\n","Episode: 4110, Frame count: 142506, Running reward: -2.43\n","Episode: 4120, Frame count: 142789, Running reward: -3.2\n","Episode: 4130, Frame count: 142982, Running reward: -2.8\n","Episode: 4140, Frame count: 143229, Running reward: -3.21\n","Episode: 4150, Frame count: 143452, Running reward: -3.15\n","Episode: 4160, Frame count: 143675, Running reward: -3.21\n","Episode: 4170, Frame count: 143916, Running reward: -3.2\n","Episode: 4180, Frame count: 144139, Running reward: -3.08\n","Episode: 4190, Frame count: 144368, Running reward: -2.98\n","Episode: 4200, Frame count: 144590, Running reward: -2.86\n","Episode: 4210, Frame count: 144791, Running reward: -2.85\n","Episode: 4220, Frame count: 145020, Running reward: -2.31\n","Episode: 4230, Frame count: 145261, Running reward: -2.79\n","Episode: 4240, Frame count: 145472, Running reward: -2.43\n","Episode: 4250, Frame count: 145693, Running reward: -2.41\n","Episode: 4260, Frame count: 145876, Running reward: -2.01\n","Episode: 4270, Frame count: 146097, Running reward: -1.81\n","Episode: 4280, Frame count: 146321, Running reward: -1.82\n","Episode: 4290, Frame count: 146562, Running reward: -1.94\n","Episode: 4300, Frame count: 146807, Running reward: -2.17\n","Episode: 4310, Frame count: 147033, Running reward: -2.42\n","Episode: 4320, Frame count: 147247, Running reward: -2.27\n","Episode: 4330, Frame count: 147488, Running reward: -2.27\n","Episode: 4340, Frame count: 147716, Running reward: -2.44\n","Episode: 4350, Frame count: 147937, Running reward: -2.44\n","Episode: 4360, Frame count: 148155, Running reward: -2.79\n","Episode: 4370, Frame count: 148378, Running reward: -2.81\n","Episode: 4380, Frame count: 148586, Running reward: -2.65\n","Episode: 4390, Frame count: 148805, Running reward: -2.43\n","Episode: 4400, Frame count: 149021, Running reward: -2.14\n","Episode: 4410, Frame count: 149229, Running reward: -1.96\n","Episode: 4420, Frame count: 149461, Running reward: -2.14\n","Episode: 4430, Frame count: 149672, Running reward: -1.84\n","Episode: 4440, Frame count: 149881, Running reward: -1.65\n","Episode: 4450, Frame count: 150088, Running reward: -1.51\n","Episode: 4460, Frame count: 150292, Running reward: -1.37\n","Episode: 4470, Frame count: 150502, Running reward: -1.24\n","Episode: 4480, Frame count: 150728, Running reward: -1.42\n","Episode: 4490, Frame count: 150925, Running reward: -1.2\n","Episode: 4500, Frame count: 151141, Running reward: -1.2\n","Episode: 4510, Frame count: 151339, Running reward: -1.1\n","Episode: 4520, Frame count: 151549, Running reward: -0.88\n","Episode: 4530, Frame count: 151754, Running reward: -0.82\n","Episode: 4540, Frame count: 151964, Running reward: -0.83\n","Episode: 4550, Frame count: 152180, Running reward: -0.92\n","Episode: 4560, Frame count: 152395, Running reward: -1.03\n","Episode: 4570, Frame count: 152587, Running reward: -0.85\n","Episode: 4580, Frame count: 152787, Running reward: -0.59\n","Episode: 4590, Frame count: 153010, Running reward: -0.85\n","Episode: 4600, Frame count: 153230, Running reward: -0.89\n","Episode: 4610, Frame count: 153457, Running reward: -1.18\n","Episode: 4620, Frame count: 153649, Running reward: -1.0\n","Episode: 4630, Frame count: 153874, Running reward: -1.2\n","Episode: 4640, Frame count: 154076, Running reward: -1.12\n","Episode: 4650, Frame count: 154303, Running reward: -1.23\n","Episode: 4660, Frame count: 154523, Running reward: -1.28\n","Episode: 4670, Frame count: 154734, Running reward: -1.47\n","Episode: 4680, Frame count: 154951, Running reward: -1.64\n","Episode: 4690, Frame count: 155156, Running reward: -1.46\n","Episode: 4700, Frame count: 155366, Running reward: -1.36\n","Episode: 4710, Frame count: 155585, Running reward: -1.28\n","Episode: 4720, Frame count: 155800, Running reward: -1.51\n","Episode: 4730, Frame count: 156004, Running reward: -1.3\n","Episode: 4740, Frame count: 156200, Running reward: -1.24\n","Episode: 4750, Frame count: 156387, Running reward: -0.84\n","Episode: 4760, Frame count: 156604, Running reward: -0.81\n","Episode: 4770, Frame count: 156834, Running reward: -1.0\n","Episode: 4780, Frame count: 157039, Running reward: -0.88\n","Episode: 4790, Frame count: 157243, Running reward: -0.87\n","Episode: 4800, Frame count: 157443, Running reward: -0.77\n","Episode: 4810, Frame count: 157643, Running reward: -0.58\n","Episode: 4820, Frame count: 157847, Running reward: -0.47\n","Episode: 4830, Frame count: 158026, Running reward: -0.22\n","Episode: 4840, Frame count: 158224, Running reward: -0.24\n","Episode: 4850, Frame count: 158432, Running reward: -0.45\n","Episode: 4860, Frame count: 158639, Running reward: -0.35\n","Episode: 4870, Frame count: 158842, Running reward: -0.08\n","Episode: 4880, Frame count: 159050, Running reward: -0.11\n","Episode: 4890, Frame count: 159235, Running reward: 0.08\n","Episode: 4900, Frame count: 159444, Running reward: -0.01\n","Episode: 4910, Frame count: 159673, Running reward: -0.3\n","Episode: 4920, Frame count: 159887, Running reward: -0.4\n","Episode: 4930, Frame count: 160083, Running reward: -0.57\n","Episode: 4940, Frame count: 160293, Running reward: -0.69\n","Episode: 4950, Frame count: 160506, Running reward: -0.74\n","Episode: 4960, Frame count: 160714, Running reward: -0.75\n","Episode: 4970, Frame count: 160927, Running reward: -0.85\n","Episode: 4980, Frame count: 161119, Running reward: -0.69\n","Episode: 4990, Frame count: 161330, Running reward: -0.95\n","Episode: 5000, Frame count: 161540, Running reward: -0.96\n","Episode: 5010, Frame count: 161756, Running reward: -0.83\n","Episode: 5020, Frame count: 161930, Running reward: -0.43\n","Episode: 5030, Frame count: 162137, Running reward: -0.54\n","Episode: 5040, Frame count: 162347, Running reward: -0.54\n","Episode: 5050, Frame count: 162557, Running reward: -0.51\n","Episode: 5060, Frame count: 162751, Running reward: -0.37\n","Episode: 5070, Frame count: 162942, Running reward: -0.15\n","Episode: 5080, Frame count: 163137, Running reward: -0.18\n","Episode: 5090, Frame count: 163336, Running reward: -0.06\n","Episode: 5100, Frame count: 163525, Running reward: 0.15\n","Episode: 5110, Frame count: 163710, Running reward: 0.46\n","Episode: 5120, Frame count: 163903, Running reward: 0.27\n","Episode: 5130, Frame count: 164116, Running reward: 0.21\n","Episode: 5140, Frame count: 164292, Running reward: 0.55\n","Episode: 5150, Frame count: 164513, Running reward: 0.44\n","Episode: 5160, Frame count: 164735, Running reward: 0.16\n","Episode: 5170, Frame count: 164929, Running reward: 0.13\n","Episode: 5180, Frame count: 165112, Running reward: 0.25\n","Episode: 5190, Frame count: 165310, Running reward: 0.26\n","Episode: 5200, Frame count: 165508, Running reward: 0.17\n","Episode: 5210, Frame count: 165720, Running reward: -0.1\n","Episode: 5220, Frame count: 165909, Running reward: -0.06\n","Episode: 5230, Frame count: 166109, Running reward: 0.07\n","Episode: 5240, Frame count: 166289, Running reward: 0.03\n","Episode: 5250, Frame count: 166502, Running reward: 0.11\n","Episode: 5260, Frame count: 166715, Running reward: 0.2\n","Episode: 5270, Frame count: 166912, Running reward: 0.17\n","Episode: 5280, Frame count: 167107, Running reward: 0.05\n","Episode: 5290, Frame count: 167314, Running reward: -0.04\n","Episode: 5300, Frame count: 167524, Running reward: -0.16\n","Episode: 5310, Frame count: 167724, Running reward: -0.04\n","Episode: 5320, Frame count: 167924, Running reward: -0.15\n","Episode: 5330, Frame count: 168124, Running reward: -0.15\n","Episode: 5340, Frame count: 168336, Running reward: -0.47\n","Episode: 5350, Frame count: 168542, Running reward: -0.4\n","Episode: 5360, Frame count: 168732, Running reward: -0.17\n","Episode: 5370, Frame count: 168931, Running reward: -0.19\n","Episode: 5380, Frame count: 169145, Running reward: -0.38\n","Episode: 5390, Frame count: 169349, Running reward: -0.35\n","Episode: 5400, Frame count: 169554, Running reward: -0.3\n","Episode: 5410, Frame count: 169755, Running reward: -0.31\n","Episode: 5420, Frame count: 169921, Running reward: 0.03\n","Episode: 5430, Frame count: 170114, Running reward: 0.1\n","Episode: 5440, Frame count: 170303, Running reward: 0.33\n","Episode: 5450, Frame count: 170489, Running reward: 0.53\n","Episode: 5460, Frame count: 170674, Running reward: 0.58\n","Episode: 5470, Frame count: 170863, Running reward: 0.68\n","Episode: 5480, Frame count: 171068, Running reward: 0.77\n","Episode: 5490, Frame count: 171266, Running reward: 0.83\n","Episode: 5500, Frame count: 171455, Running reward: 0.99\n","Episode: 5510, Frame count: 171636, Running reward: 1.19\n","Episode: 5520, Frame count: 171821, Running reward: 1.0\n","Episode: 5530, Frame count: 172020, Running reward: 0.94\n","Episode: 5540, Frame count: 172223, Running reward: 0.8\n","Episode: 5550, Frame count: 172422, Running reward: 0.67\n","Episode: 5560, Frame count: 172618, Running reward: 0.56\n","Episode: 5570, Frame count: 172801, Running reward: 0.62\n","Episode: 5580, Frame count: 172985, Running reward: 0.83\n","Episode: 5590, Frame count: 173189, Running reward: 0.77\n","Episode: 5600, Frame count: 173378, Running reward: 0.77\n","Episode: 5610, Frame count: 173571, Running reward: 0.65\n","Episode: 5620, Frame count: 173749, Running reward: 0.72\n","Episode: 5630, Frame count: 173929, Running reward: 0.91\n","Episode: 5640, Frame count: 174112, Running reward: 1.11\n","Episode: 5650, Frame count: 174306, Running reward: 1.16\n","Episode: 5660, Frame count: 174490, Running reward: 1.28\n","Episode: 5670, Frame count: 174689, Running reward: 1.12\n","Episode: 5680, Frame count: 174902, Running reward: 0.83\n","Episode: 5690, Frame count: 175096, Running reward: 0.93\n","Episode: 5700, Frame count: 175281, Running reward: 0.97\n","Episode: 5710, Frame count: 175479, Running reward: 0.92\n","Episode: 5720, Frame count: 175659, Running reward: 0.9\n","Episode: 5730, Frame count: 175828, Running reward: 1.01\n","Episode: 5740, Frame count: 176015, Running reward: 0.97\n","Episode: 5750, Frame count: 176203, Running reward: 1.03\n","Episode: 5760, Frame count: 176388, Running reward: 1.02\n","Episode: 5770, Frame count: 176560, Running reward: 1.29\n","Episode: 5780, Frame count: 176758, Running reward: 1.44\n","Episode: 5790, Frame count: 176934, Running reward: 1.62\n","Episode: 5800, Frame count: 177113, Running reward: 1.68\n","Episode: 5810, Frame count: 177298, Running reward: 1.81\n","Episode: 5820, Frame count: 177477, Running reward: 1.82\n","Episode: 5830, Frame count: 177632, Running reward: 1.96\n","Episode: 5840, Frame count: 177841, Running reward: 1.74\n","Episode: 5850, Frame count: 178004, Running reward: 1.99\n","Episode: 5860, Frame count: 178198, Running reward: 1.9\n","Episode: 5870, Frame count: 178402, Running reward: 1.58\n","Episode: 5880, Frame count: 178582, Running reward: 1.76\n","Episode: 5890, Frame count: 178762, Running reward: 1.72\n","Episode: 5900, Frame count: 178955, Running reward: 1.58\n","Episode: 5910, Frame count: 179133, Running reward: 1.65\n","Episode: 5920, Frame count: 179308, Running reward: 1.69\n","Episode: 5930, Frame count: 179482, Running reward: 1.5\n","Episode: 5940, Frame count: 179673, Running reward: 1.68\n","Episode: 5950, Frame count: 179854, Running reward: 1.5\n","Episode: 5960, Frame count: 180024, Running reward: 1.74\n","Episode: 5970, Frame count: 180189, Running reward: 2.13\n","Episode: 5980, Frame count: 180351, Running reward: 2.31\n","Episode: 5990, Frame count: 180523, Running reward: 2.39\n","Episode: 6000, Frame count: 180700, Running reward: 2.55\n","Episode: 6010, Frame count: 180889, Running reward: 2.44\n","Episode: 6020, Frame count: 181063, Running reward: 2.45\n","Episode: 6030, Frame count: 181244, Running reward: 2.38\n","Episode: 6040, Frame count: 181405, Running reward: 2.68\n","Episode: 6050, Frame count: 181560, Running reward: 2.94\n","Episode: 6060, Frame count: 181738, Running reward: 2.86\n","Episode: 6070, Frame count: 181911, Running reward: 2.78\n","Episode: 6080, Frame count: 182097, Running reward: 2.54\n","Episode: 6090, Frame count: 182287, Running reward: 2.36\n","Episode: 6100, Frame count: 182484, Running reward: 2.16\n","Episode: 6110, Frame count: 182655, Running reward: 2.34\n","Episode: 6120, Frame count: 182819, Running reward: 2.44\n","Episode: 6130, Frame count: 182983, Running reward: 2.61\n","Episode: 6140, Frame count: 183149, Running reward: 2.56\n","Episode: 6150, Frame count: 183328, Running reward: 2.32\n","Episode: 6160, Frame count: 183513, Running reward: 2.25\n","Episode: 6170, Frame count: 183692, Running reward: 2.19\n","Episode: 6180, Frame count: 183872, Running reward: 2.25\n","Episode: 6190, Frame count: 184047, Running reward: 2.4\n","Episode: 6200, Frame count: 184200, Running reward: 2.84\n","Episode: 6210, Frame count: 184358, Running reward: 2.97\n","Episode: 6220, Frame count: 184523, Running reward: 2.96\n","Episode: 6230, Frame count: 184722, Running reward: 2.61\n","Episode: 6240, Frame count: 184907, Running reward: 2.42\n","Episode: 6250, Frame count: 185095, Running reward: 2.33\n","Episode: 6260, Frame count: 185259, Running reward: 2.54\n","Episode: 6270, Frame count: 185430, Running reward: 2.62\n","Episode: 6280, Frame count: 185604, Running reward: 2.68\n","Episode: 6290, Frame count: 185784, Running reward: 2.63\n","Episode: 6300, Frame count: 185945, Running reward: 2.55\n","Episode: 6310, Frame count: 186099, Running reward: 2.59\n","Episode: 6320, Frame count: 186280, Running reward: 2.43\n","Episode: 6330, Frame count: 186455, Running reward: 2.67\n","Episode: 6340, Frame count: 186621, Running reward: 2.86\n","Episode: 6350, Frame count: 186794, Running reward: 3.01\n","Episode: 6360, Frame count: 186959, Running reward: 3.0\n","Episode: 6370, Frame count: 187136, Running reward: 2.94\n","Episode: 6380, Frame count: 187318, Running reward: 2.86\n","Episode: 6390, Frame count: 187476, Running reward: 3.08\n","Episode: 6400, Frame count: 187633, Running reward: 3.12\n","Episode: 6410, Frame count: 187829, Running reward: 2.7\n","Episode: 6420, Frame count: 187993, Running reward: 2.87\n","Episode: 6430, Frame count: 188158, Running reward: 2.97\n","Episode: 6440, Frame count: 188326, Running reward: 2.95\n","Episode: 6450, Frame count: 188495, Running reward: 2.99\n","Episode: 6460, Frame count: 188659, Running reward: 3.0\n","Episode: 6470, Frame count: 188829, Running reward: 3.07\n","Episode: 6480, Frame count: 188987, Running reward: 3.31\n","Episode: 6490, Frame count: 189168, Running reward: 3.08\n","Episode: 6500, Frame count: 189324, Running reward: 3.09\n","Episode: 6510, Frame count: 189496, Running reward: 3.33\n","Episode: 6520, Frame count: 189680, Running reward: 3.13\n","Episode: 6530, Frame count: 189830, Running reward: 3.28\n","Episode: 6540, Frame count: 190003, Running reward: 3.23\n","Episode: 6550, Frame count: 190168, Running reward: 3.27\n","Episode: 6560, Frame count: 190333, Running reward: 3.26\n","Episode: 6570, Frame count: 190507, Running reward: 3.22\n","Episode: 6580, Frame count: 190669, Running reward: 3.18\n","Episode: 6590, Frame count: 190824, Running reward: 3.44\n","Episode: 6600, Frame count: 190981, Running reward: 3.43\n","Episode: 6610, Frame count: 191159, Running reward: 3.37\n","Episode: 6620, Frame count: 191299, Running reward: 3.81\n","Episode: 6630, Frame count: 191464, Running reward: 3.66\n","Episode: 6640, Frame count: 191633, Running reward: 3.7\n","Episode: 6650, Frame count: 191800, Running reward: 3.68\n","Episode: 6660, Frame count: 191953, Running reward: 3.8\n","Episode: 6670, Frame count: 192121, Running reward: 3.86\n","Episode: 6680, Frame count: 192293, Running reward: 3.76\n","Episode: 6690, Frame count: 192454, Running reward: 3.7\n","Episode: 6700, Frame count: 192638, Running reward: 3.43\n","Episode: 6710, Frame count: 192805, Running reward: 3.54\n","Episode: 6720, Frame count: 192958, Running reward: 3.41\n","Episode: 6730, Frame count: 193124, Running reward: 3.4\n","Episode: 6740, Frame count: 193292, Running reward: 3.41\n","Episode: 6750, Frame count: 193456, Running reward: 3.44\n","Episode: 6760, Frame count: 193627, Running reward: 3.26\n","Episode: 6770, Frame count: 193787, Running reward: 3.34\n","Episode: 6780, Frame count: 193951, Running reward: 3.42\n","Episode: 6790, Frame count: 194110, Running reward: 3.44\n","Episode: 6800, Frame count: 194269, Running reward: 3.69\n","Episode: 6810, Frame count: 194429, Running reward: 3.76\n","Episode: 6820, Frame count: 194591, Running reward: 3.67\n","Episode: 6830, Frame count: 194749, Running reward: 3.75\n","Episode: 6840, Frame count: 194915, Running reward: 3.77\n","Episode: 6850, Frame count: 195088, Running reward: 3.68\n","Episode: 6860, Frame count: 195242, Running reward: 3.85\n","Episode: 6870, Frame count: 195407, Running reward: 3.8\n","Episode: 6880, Frame count: 195563, Running reward: 3.88\n","Episode: 6890, Frame count: 195744, Running reward: 3.66\n","Episode: 6900, Frame count: 195909, Running reward: 3.6\n","Episode: 6910, Frame count: 196089, Running reward: 3.4\n","Episode: 6920, Frame count: 196249, Running reward: 3.42\n","Episode: 6930, Frame count: 196405, Running reward: 3.44\n","Episode: 6940, Frame count: 196561, Running reward: 3.54\n","Episode: 6950, Frame count: 196728, Running reward: 3.6\n","Episode: 6960, Frame count: 196894, Running reward: 3.48\n","Episode: 6970, Frame count: 197053, Running reward: 3.54\n","Episode: 6980, Frame count: 197227, Running reward: 3.36\n","Episode: 6990, Frame count: 197388, Running reward: 3.56\n","Episode: 7000, Frame count: 197543, Running reward: 3.66\n","Episode: 7010, Frame count: 197706, Running reward: 3.83\n","Episode: 7020, Frame count: 197870, Running reward: 3.79\n","Episode: 7030, Frame count: 198026, Running reward: 3.79\n","Episode: 7040, Frame count: 198183, Running reward: 3.78\n","Episode: 7050, Frame count: 198345, Running reward: 3.83\n","Episode: 7060, Frame count: 198492, Running reward: 4.02\n","Episode: 7070, Frame count: 198661, Running reward: 3.92\n","Episode: 7080, Frame count: 198817, Running reward: 4.1\n","Episode: 7090, Frame count: 198995, Running reward: 3.93\n","Episode: 7100, Frame count: 199160, Running reward: 3.83\n","Episode: 7110, Frame count: 199310, Running reward: 3.96\n","Episode: 7120, Frame count: 199487, Running reward: 3.83\n","Episode: 7130, Frame count: 199656, Running reward: 3.7\n","Episode: 7140, Frame count: 199810, Running reward: 3.73\n","Episode: 7150, Frame count: 199965, Running reward: 3.8\n","Episode: 7160, Frame count: 200118, Running reward: 3.74\n","Episode: 7170, Frame count: 200265, Running reward: 3.96\n","Episode: 7180, Frame count: 200408, Running reward: 4.09\n","Episode: 7190, Frame count: 200559, Running reward: 4.36\n","Episode: 7200, Frame count: 200719, Running reward: 4.41\n","Episode: 7210, Frame count: 200879, Running reward: 4.31\n","Episode: 7220, Frame count: 201034, Running reward: 4.53\n","Episode: 7230, Frame count: 201207, Running reward: 4.49\n","Episode: 7240, Frame count: 201384, Running reward: 4.26\n","Episode: 7250, Frame count: 201537, Running reward: 4.28\n","Episode: 7260, Frame count: 201694, Running reward: 4.24\n","Episode: 7270, Frame count: 201858, Running reward: 4.07\n","Episode: 7280, Frame count: 202022, Running reward: 3.86\n","Episode: 7290, Frame count: 202188, Running reward: 3.71\n","Episode: 7300, Frame count: 202350, Running reward: 3.69\n","Episode: 7310, Frame count: 202519, Running reward: 3.6\n","Episode: 7320, Frame count: 202684, Running reward: 3.5\n","Episode: 7330, Frame count: 202851, Running reward: 3.56\n","Episode: 7340, Frame count: 203007, Running reward: 3.77\n","Episode: 7350, Frame count: 203176, Running reward: 3.61\n","Episode: 7360, Frame count: 203359, Running reward: 3.35\n","Episode: 7370, Frame count: 203530, Running reward: 3.28\n","Episode: 7380, Frame count: 203695, Running reward: 3.27\n","Episode: 7390, Frame count: 203852, Running reward: 3.36\n","Episode: 7400, Frame count: 204002, Running reward: 3.48\n","Episode: 7410, Frame count: 204150, Running reward: 3.69\n","Episode: 7420, Frame count: 204322, Running reward: 3.62\n","Episode: 7430, Frame count: 204493, Running reward: 3.58\n","Episode: 7440, Frame count: 204644, Running reward: 3.63\n","Episode: 7450, Frame count: 204801, Running reward: 3.75\n","Episode: 7460, Frame count: 204963, Running reward: 3.96\n","Episode: 7470, Frame count: 205111, Running reward: 4.19\n","Episode: 7480, Frame count: 205281, Running reward: 4.14\n","Episode: 7490, Frame count: 205453, Running reward: 3.99\n","Episode: 7500, Frame count: 205608, Running reward: 3.94\n"]}],"source":["for _ in range(max_episodes):\n","    state, info = env.reset()\n","    state = preprocess_state(state)\n","    action_mask = info['action_mask'].reshape((-1,))\n","    episode_reward = 0\n","\n","    for timestep in range(1, max_steps_per_episode):\n","        frame_count += 1\n","\n","        # Select an action\n","        #state_cuda = state.to(device)\n","        action = get_greedy_epsilon(model, state, action_mask)\n","        if action < 0:\n","            print(action_mask)\n","\n","        # Take the selected action\n","        state_next, reward, done, _, info = env.step((action // 8, action % 8))\n","        state_next = preprocess_state(state_next)\n","        action_mask = info['action_mask'].reshape((-1,))\n","\n","        episode_reward += reward\n","\n","        # Store the transition in the replay buffer\n","        action_history.append(action)\n","        action_mask_history.append(action_mask)\n","        state_history.append(state)\n","        state_next_history.append(state_next)\n","        rewards_history.append(reward)\n","        done_history.append(done)\n","\n","        state = state_next\n","\n","        # Update every fourth frame and once batch size is over 32\n","        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n","            update_network()\n","\n","        if frame_count % update_target_network == 0:\n","            model_target.load_state_dict(model.state_dict())\n","\n","        # Limit the state and reward history\n","        if len(rewards_history) > max_memory_length:\n","            del rewards_history[:1]\n","            del state_history[:1]\n","            del state_next_history[:1]\n","            del action_history[:1]\n","            del action_mask_history[:1]\n","            del done_history[:1]\n","\n","        if done:\n","            break\n","\n","    episode_count += 1\n","    episode_reward_history.append(episode_reward)\n","\n","    # Update running reward to check condition for solving\n","    if len(episode_reward_history) > 100:\n","        del episode_reward_history[:1]\n","    running_reward = np.mean(episode_reward_history)\n","\n","    if episode_count % 10 == 0:\n","        print(f\"Episode: {episode_count}, Frame count: {frame_count}, Running reward: {running_reward}\")\n","\n","    if episode_count % 5000 == 0:\n","        torch.save(model, 'model.{}'.format(episode_count))\n","    #if running_reward > 20:\n","    #    print(f\"Solved at episode {episode_count}!\")\n","    #    break\n","\n","\n","torch.save(model, 'model.final')"]},{"cell_type":"markdown","id":"4984b880-e427-48cb-bf91-13a91d6529f5","metadata":{"id":"4984b880-e427-48cb-bf91-13a91d6529f5"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":58,"id":"5c198b4e-b0e4-4fd4-821d-be602c2dbc5a","metadata":{"id":"5c198b4e-b0e4-4fd4-821d-be602c2dbc5a","executionInfo":{"status":"ok","timestamp":1721705927258,"user_tz":-540,"elapsed":12172,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0f7a4ded-3eae-4494-ed1a-d4fe47b91105"},"outputs":[{"output_type":"stream","name":"stdout","text":["         |         \n","   M H   |      H  \n","   H H   |    H H  \n","   HHHH  |    HHHH \n","  MH H   |    H H  \n","     H   |      H  \n","         |         \n","         |         \n","\n"]}],"source":["import time, sys\n","from IPython.display import clear_output\n","\n","board, info = env.reset()\n","state = preprocess_state(board)\n","action_mask = info['action_mask'].reshape((-1,))\n","done = False\n","env.render()\n","\n","while not done:\n","    action = get_greedy_action(model, state, action_mask)\n","    print(\"action: ({}, {})\".format(action // 8, action % 8))\n","    sys.stdout.flush()\n","\n","    time.sleep(1.0)\n","    clear_output(wait=False)\n","    board, reward, done, _, info = env.step((action // 8, action % 8))\n","    state = preprocess_state(board)\n","    action_mask = info['action_mask'].reshape((-1,))\n","    env.render()"]},{"cell_type":"code","source":[],"metadata":{"id":"bIQ5bXmNZ-0C"},"id":"bIQ5bXmNZ-0C","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}