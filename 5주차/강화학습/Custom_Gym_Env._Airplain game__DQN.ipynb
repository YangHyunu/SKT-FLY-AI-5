{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1w364KROOTXg7JTepnA76wDNuzhAsj0sJ","authorship_tag":"ABX9TyNsTVrfi5wd84Y0zObhIpQU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qN99eNa6CN-i","executionInfo":{"status":"ok","timestamp":1721722738659,"user_tz":-540,"elapsed":3288,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"dcf1dc1d-a917-497f-e190-7ac7b73a149e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q1uW83uLCc72","executionInfo":{"status":"ok","timestamp":1721722738660,"user_tz":-540,"elapsed":24,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"e15940a6-a147-4a80-b85a-633808e47584"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["drive  gym_examples  sample_data\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"J85OGqZpfUAj","executionInfo":{"status":"ok","timestamp":1721722738661,"user_tz":-540,"elapsed":16,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["!cp -r /content/drive/MyDrive/시리즈/gym_examples/gym_examples ."],"metadata":{"id":"Vjo0lXBICjMS","executionInfo":{"status":"ok","timestamp":1721722738661,"user_tz":-540,"elapsed":14,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"YGM1x6eJaqc8","executionInfo":{"status":"ok","timestamp":1721722738661,"user_tz":-540,"elapsed":13,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["\n","!ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZm5hEEvCspa","executionInfo":{"status":"ok","timestamp":1721722739176,"user_tz":-540,"elapsed":527,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"15aa8672-206f-4df6-f2a7-8dad2daf53df"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["drive  gym_examples  sample_data\n"]}]},{"cell_type":"code","source":["!pip install gymnasium"],"metadata":{"id":"3j1_Y_oVCtuj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721722745659,"user_tz":-540,"elapsed":6489,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"1b38c7fa-16f5-4900-d1df-23ef3ad54e55"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n","Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"]}]},{"cell_type":"code","source":["import gymnasium as gym\n","env = gym.make('gym_examples:gym_examples/ShootingAirplane-v0', render_mode='text')"],"metadata":{"id":"4MebXsRGF-7u","executionInfo":{"status":"ok","timestamp":1721722745659,"user_tz":-540,"elapsed":15,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["obs, info = env.reset()"],"metadata":{"id":"b5h6Rt36G1R3","executionInfo":{"status":"ok","timestamp":1721722748208,"user_tz":-540,"elapsed":549,"user":{"displayName":"양현우","userId":"16626705175391526571"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["env.render() #  화면 표시"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e0UP7mQ8G3f5","executionInfo":{"status":"ok","timestamp":1721722748210,"user_tz":-540,"elapsed":7,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"9797acce-d3db-4b77-8611-ca08dbc870ff"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["         |         \n","         |         \n","         |         \n","         |     H   \n","         |   H H   \n","         |   HHHH  \n","         |   H H   \n","         |     H   \n","\n"]}]},{"cell_type":"code","source":["obs, reward, done,_, info = env.step((0, 0))"],"metadata":{"id":"CXqsxzItG5RH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env.render()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"csYQTIUfG77B","executionInfo":{"status":"ok","timestamp":1721696512772,"user_tz":-540,"elapsed":355,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"eaf30bdd-c3af-4b10-d363-168df571b529"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["M        |   H     \n","         | HHHHH   \n","         |   H     \n","         |  HHH    \n","         |         \n","         |         \n","         |         \n","         |         \n","\n"]}]},{"cell_type":"code","source":["obs, reward, done, _,info = env.step((3, 5))"],"metadata":{"id":"wqsxnL-jG-ef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env.render()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7R2b-0TYHBIi","executionInfo":{"status":"ok","timestamp":1721696530682,"user_tz":-540,"elapsed":377,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"c4c87f65-50dd-4d43-928f-c6cbc8a9055c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["M        |   H     \n","         | HHHHH   \n","         |   H     \n","     M   |  HHH    \n","         |         \n","         |         \n","         |         \n","         |         \n","\n"]}]},{"cell_type":"code","source":["obs, reward, done,_, info = env.step((3, 3))\n","env.render()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Aszo_qhFHC2q","executionInfo":{"status":"ok","timestamp":1721696541285,"user_tz":-540,"elapsed":1010,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"96a66a4f-2296-41b6-9c18-0f249834a9c4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["M        |   H     \n","         | HHHHH   \n","         |   H     \n","   H M   |  HHH    \n","         |         \n","         |         \n","         |         \n","         |         \n","\n"]}]},{"cell_type":"code","source":["reward"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"82NM-GhKHFSQ","executionInfo":{"status":"ok","timestamp":1721696559520,"user_tz":-540,"elapsed":382,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"804657f7-643d-4a6e-e072-eef941cc512b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["obs, reward, done, _, info = env.step((5, 4))\n","env.render()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qocEvY6KHJ5c","executionInfo":{"status":"ok","timestamp":1721696566148,"user_tz":-540,"elapsed":358,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"5719cda4-16c5-4763-e198-e2fb4f3c2bd1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["M        |   H     \n","         | HHHHH   \n","         |   H     \n","   H M   |  HHH    \n","         |         \n","    M    |         \n","         |         \n","         |         \n","\n"]}]},{"cell_type":"code","source":["reward"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZRLWbOx7HLhT","executionInfo":{"status":"ok","timestamp":1721696576051,"user_tz":-540,"elapsed":359,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"b0ec842e-0f31-4183-abe2-f1729d43cbe8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["-1"]},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","source":["- 현재 셀 까지는 Gym 커스텀 환경만들고 설정하여 게임이 제대로 구동돼는지 확인했음  "],"metadata":{"id":"1pCBa5RgHN8U"}},{"cell_type":"markdown","source":["# DQN\n","-  Q-learning을 심층 신경망(Deep Neural Network)과 결합한 방법"],"metadata":{"id":"z4lXq27aMiMI"}},{"cell_type":"code","source":["import numpy as np\n","import gymnasium as gym\n","import torch\n","import torch.nn as nn\n","import torchvision.transforms as T\n","\n","# Configuration paramaters for the whole setup\n","seed = 42\n","gamma = 0.99  # Discount factor for past rewards\n","epsilon = 1.0  # Epsilon greedy parameter\n","epsilon_min = 0.1  # Minimum epsilon greedy parameter\n","epsilon_max = 1.0  # Maximum epsilon greedy parameter\n","epsilon_interval = epsilon_max - epsilon_min # Rate at which to reduce chance\n","                                             # of random action being taken\n","batch_size = 16  # Size of batch taken from replay buffer\n","max_steps_per_episode = 60\n","max_episodes = 30000"],"metadata":{"id":"vRPAnQYcNkIX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env.action_space.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SrLQtxURbg4C","executionInfo":{"status":"ok","timestamp":1721702960760,"user_tz":-540,"elapsed":15,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"a37d57dd-661f-403b-e81f-c7b5ed537c2a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2,)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["env.observation_space.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-J5GQX8eblWA","executionInfo":{"status":"ok","timestamp":1721702960761,"user_tz":-540,"elapsed":14,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"9afa6610-7342-43b1-b0c9-f259c72d535c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8, 8, 1)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","num_actions = 64\n","\n","class QModel(nn.Module):\n","    def __init__(self, num_actions):\n","        super(QModel, self).__init__()\n","        self.dropout = nn.Dropout(p=0.3)\n","        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding='same')\n","        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding='same')\n","        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1)\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(1152, 512)\n","        self.fc2 = nn.Linear(512, num_actions)\n","\n","    def forward(self, x):\n","        x = nn.functional.relu(self.conv1(x))\n","        x = nn.functional.relu(self.conv2(x))\n","        x = self.dropout(x)\n","        x = nn.functional.relu(self.conv3(x))\n","        x = self.flatten(x)\n","        x = nn.functional.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        action = self.fc2(x)\n","        return action"],"metadata":{"id":"VZW_N47wbndV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# The first model makes the predictions for Q-values which are used to\n","# make a action.\n","model = QModel(num_actions)\n","model.to(device)\n","\n","# Build a target model for the prediction of future rewards.\n","# The weights of a target model get updated every 10000 steps thus when the\n","# loss between the Q-values is calculated the target Q-value is stable.\n","model_target = QModel(num_actions)\n","model_target.to(device)\n","\n","loss_function = nn.SmoothL1Loss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.00025)"],"metadata":{"id":"eRFfYK9Tbpvz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1tcH6fdqbrma","executionInfo":{"status":"ok","timestamp":1721702961193,"user_tz":-540,"elapsed":440,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"723b49aa-6a40-472e-c157-9bd0fd4ffd1f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda', index=0)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# Experience replay buffers\n","action_history = []\n","action_mask_history = []\n","state_history = []\n","state_next_history = []\n","rewards_history = []\n","done_history = []\n","episode_reward_history = []\n","running_reward = 0\n","episode_count = 0\n","frame_count = 0\n","\n","# Number of frames to take random action and observe output\n","epsilon_random_frames = 50000\n","# Number of frames for exploration\n","epsilon_greedy_frames = 200000.0\n","# Maximum replay length\n","# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n","max_memory_length = 500000\n","# Train the model after 4 actions\n","update_after_actions = 4\n","# How often to update the target network\n","update_target_network = 10000"],"metadata":{"id":"BOW-TCOBbt2p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to preprocess the state\n","# note that player 1 = env player, player 2 = agent\n","def preprocess_state(env_observ):\n","    st = torch.from_numpy(env_observ).squeeze()\n","    st = st.to(torch.int64)\n","    st = torch.nn.functional.one_hot(st,num_classes=3)\n","    st = st.permute(2, 0, 1)\n","    return st.to(torch.float32)"],"metadata":{"id":"TwEfUrt9bwKp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["board, info = env.reset()"],"metadata":{"id":"-fMrcRMSbx8B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["board"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_MHFxum-b0Fg","executionInfo":{"status":"ok","timestamp":1721702961196,"user_tz":-540,"elapsed":36,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"14279cb1-4598-48f0-bbe3-72bcb78cf34d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]],\n","\n","       [[0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0],\n","        [0]]], dtype=uint8)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["info['action_mask']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1wE1KXYBb1so","executionInfo":{"status":"ok","timestamp":1721702961198,"user_tz":-540,"elapsed":30,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"9eceab13-0903-423d-c810-f72026754e50"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1],\n","       [1, 1, 1, 1, 1, 1, 1, 1]], dtype=int32)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["state = preprocess_state(board)"],"metadata":{"id":"vtqh2J_zb4nE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zvy29uvEb6tk","executionInfo":{"status":"ok","timestamp":1721702961199,"user_tz":-540,"elapsed":25,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"a9aff9a6-e6a7-4e26-fff3-75bc6f40c68d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 1., 1., 1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1., 1., 1., 1.],\n","         [1., 1., 1., 1., 1., 1., 1., 1.]],\n","\n","        [[0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.]],\n","\n","        [[0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.],\n","         [0., 0., 0., 0., 0., 0., 0., 0.]]])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["state.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9YSbas2Cb90_","executionInfo":{"status":"ok","timestamp":1721702961200,"user_tz":-540,"elapsed":22,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"outputId":"b50f0b6d-cf1b-42a8-e57c-0ea919c69fc1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 8, 8])"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["with torch.no_grad():\n","    model_output = model(state.unsqueeze(0))"],"metadata":{"id":"whQVlVdxb9yo","executionInfo":{"status":"error","timestamp":1721702970556,"user_tz":-540,"elapsed":553,"user":{"displayName":"양현우","userId":"16626705175391526571"}},"colab":{"base_uri":"https://localhost:8080/","height":377},"outputId":"31aba62c-2f92-496b-e7ab-74ab69e0a5b0"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper_CUDA___slow_conv2d_forward)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-f1b5775f71dc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-b940cf7a1202>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    454\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 456\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    457\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper_CUDA___slow_conv2d_forward)"]}]},{"cell_type":"code","source":["model_output.shape"],"metadata":{"id":"VNMOFl2Xb9wZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to select an action\n","# model: the torch model to compuate action-state value (i.e., q-value)\n","# state: a torch tensor (3 x 8 x 8) of float32, which is output by preprocess_state\n","# mask: a 64-size array (np.array)\n","def get_greedy_epsilon(model, state, mask):\n","    global epsilon\n","\n","    #if frame_count < epsilon_random_frames or np.random.rand(1)[0] < epsilon:\n","    if np.random.rand(1)[0] < epsilon:\n","        action = np.random.choice([ i for i in range(num_actions) if mask[i] == 1 ])\n","    else:\n","        with torch.no_grad():\n","            # add a batch axis\n","            state_tensor = state.unsqueeze(0)\n","            # compute the q-values\n","            q_values = model(state_tensor)\n","            # select the q-values of valid actions\n","            action = torch.argmax(\n","                q_values.squeeze() + torch.from_numpy(mask) * 100., # trick to select a valid action\n","                dim=0)\n","\n","\n","            #valid_q = [ (i, q_values[0][i]) for i in range(64) if mask[i] == 1 ]\n","            # the action of maximum q-value\n","            #action, _ = max(valid_q, key=lambda e: e[1])\n","\n","    # decay epsilon\n","    epsilon -= epsilon_interval / epsilon_greedy_frames\n","    epsilon = max(epsilon, epsilon_min)\n","\n","    return action"],"metadata":{"id":"Qd97GaFub9t_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mask = np.zeros((64,), dtype=np.int64)\n","mask[12] = 1.\n","get_greedy_epsilon(model, state, mask)"],"metadata":{"id":"X7tu-zwdcEvu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_greedy_action(model, state, mask):\n","    global epsilon\n","\n","    with torch.no_grad():\n","        state_tensor = state.unsqueeze(0) # batch dimension\n","        q_values = model(state_tensor)\n","\n","        action = torch.argmax(\n","                q_values.squeeze() + torch.from_numpy(mask) * 100., # trick to select a valid action\n","                dim=0)\n","\n","    return action"],"metadata":{"id":"lDXgw-p6cGa9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sample a batch of _batch_size from replay buffers\n","# return numpy.ndarrays\n","def sample_batch(_batch_size):\n","    # Get indices of samples for replay buffers\n","    indices = np.random.choice(range(len(done_history)), size=_batch_size, replace=False)\n","\n","    state_sample = np.array([state_history[i].squeeze(0).numpy() for i in indices])\n","    state_next_sample = np.array([state_next_history[i].squeeze(0).numpy() for i in indices])\n","    rewards_sample = np.array([rewards_history[i] for i in indices], dtype=np.float32)\n","    action_sample = np.array([action_history[i] for i in indices])\n","\n","    # action mask is the mask for the valid actions at the '''next''' state\n","    action_mask_sample = np.array([action_mask_history[i] for i in indices])\n","    done_sample = np.array([float(done_history[i]) for i in indices])\n","\n","    return state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample"],"metadata":{"id":"-0IhBrarcH3P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Function to update the Q-network\n","def update_network():\n","    # sample a batch of ...\n","    state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample = \\\n","        sample_batch(batch_size)\n","\n","    # Convert numpy arrays to PyTorch tensors\n","    state_sample = torch.tensor(state_sample, dtype=torch.float32).to(device)\n","    state_next_sample = torch.tensor(state_next_sample, dtype=torch.float32).to(device)\n","    action_sample = torch.tensor(action_sample, dtype=torch.int64).to(device)\n","    action_mask_sample = torch.tensor(action_mask_sample, dtype=torch.int64).to(device)\n","    rewards_sample = torch.tensor(rewards_sample, dtype=torch.float32).to(device)\n","    done_sample = torch.tensor(done_sample, dtype=torch.float32).to(device)\n","\n","    # Compute the target Q-values for the states\n","    with torch.no_grad():\n","        future_rewards = model_target(state_next_sample)\n","        #future_rewards = future_rewards.cpu()\n","\n","        # compute the q-value for the next state and the action maximizing the q-value\n","        # note: the action should be valid (i.e., mask is set to 1)\n","        max_q_values = torch.max(\n","            future_rewards + action_mask_sample * 100., # trick to select a valid action\n","            dim=1).values.detach() - 100.\n","\n","        # compute the target q-value\n","        # if the step was final, max_q_values should not be added\n","        # we assume that the negative return of the opposite player is the return of next step\n","        # that is, G(t) = r(t+1) - g*r(t+2) + g^2*r(t+3) - g^3*r(t+4) + ...\n","        target_q_values = rewards_sample + gamma * max_q_values * (1. - done_sample)\n","\n","    # It's forward propagation! Compute the Q-values for the taken actions\n","    q_values = model(state_sample)\n","    #q_values = q_values.cpu()\n","    q_values_action = q_values.gather(dim=1, index=action_sample.unsqueeze(1)).squeeze(1)\n","\n","    # Compute the loss\n","    loss = loss_function(q_values_action, target_q_values)\n","\n","    # Perform the optimization step\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()"],"metadata":{"id":"rXCIgZYDcJb5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["board, info = env.reset()\n","board.shape"],"metadata":{"id":"GolFT_tKcK7-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["info['action_mask'].shape"],"metadata":{"id":"lplmvSgwcNB9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state = preprocess_state(board)\n","state[0] # the positions of stones of the agent player"],"metadata":{"id":"dnQ3LiNdcOTn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["action_mask = info['action_mask'].reshape((-1,))\n","action_mask.shape"],"metadata":{"id":"ubYw2Qe9cQi4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["action = get_greedy_epsilon(model, state, action_mask)\n","print(action)"],"metadata":{"id":"zhKf5Y_IcSjm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["board, reward, done, _, info = env.step((action // 8, action % 8))\n","state_next = preprocess_state(board)\n","action_mask = info['action_mask'].reshape((-1,))\n","state_next.shape"],"metadata":{"id":"ZIa7y8NScUBx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["action_mask"],"metadata":{"id":"siOEXeVAcVo5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["reward"],"metadata":{"id":"PA2ZJrtMcW0V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save actions and states in replay buffer\n","action_history.append(action)\n","action_mask_history.append(action_mask)\n","state_history.append(state)\n","state_next_history.append(state_next)\n","done_history.append(done)\n","rewards_history.append(reward)\n","\n","state = state_next"],"metadata":{"id":"fd54bKhjcX95"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get an action by epsilon-greedy policy\n","action = get_greedy_epsilon(model, state, action_mask)\n","action"],"metadata":{"id":"ovJUJGXecZWb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["board, reward, done, _, info = env.step((action // 8, action % 8))\n","state_next = preprocess_state(board)\n","action_mask = info['action_mask'].reshape((-1,))\n","state_next.shape"],"metadata":{"id":"0NPV9vVscbYH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["action_history.append(action)\n","action_mask_history.append(action_mask)\n","state_history.append(state)\n","state_next_history.append(state_next)\n","done_history.append(done)\n","rewards_history.append(reward)"],"metadata":{"id":"0y25twIFccsq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state_sample, state_next_sample, rewards_sample, action_sample, action_mask_sample, done_sample = \\\n","    sample_batch(2)"],"metadata":{"id":"rT7UamVlceLa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state_sample.shape"],"metadata":{"id":"DYt0X-l5cf13"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state_next_sample.shape"],"metadata":{"id":"XvPcANkRchSr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rewards_sample.shape"],"metadata":{"id":"HCm7G9s1cjEE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["action_sample.shape"],"metadata":{"id":"hFLu-iVackfv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["action_mask_sample.shape"],"metadata":{"id":"dg2HnsMTcl2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert numpy arrays to PyTorch tensors\n","state_sample = torch.tensor(state_sample, dtype=torch.float32)\n","state_next_sample = torch.tensor(state_next_sample, dtype=torch.float32)\n","action_sample = torch.tensor(action_sample, dtype=torch.int64)\n","action_mask_sample = torch.tensor(action_mask_sample, dtype=torch.int64)\n","rewards_sample = torch.tensor(rewards_sample, dtype=torch.float32)\n","done_sample = torch.tensor(done_sample, dtype=torch.float32)"],"metadata":{"id":"3d__oGMXcnUF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","    future_rewards = model_target(state_next_sample.to(device))\n","\n","future_rewards.shape"],"metadata":{"id":"Jvwdmjy0cpnp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["action_sample"],"metadata":{"id":"_mTjsyZccrBR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.max(future_rewards, dim=1)"],"metadata":{"id":"Rj39_Ubncta8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.max(\n","        future_rewards + action_mask_sample.to(device) * 100.,\n","        dim=1)"],"metadata":{"id":"Lho2T369cuqI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.max(\n","        future_rewards + action_mask_sample.to(device) * 100.,\n","        dim=1).values.detach() - 100."],"metadata":{"id":"kWh5Y9Kecv4u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["done_sample.unsqueeze(1)"],"metadata":{"id":"75BS7E8rcxH4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["with torch.no_grad():\n","    future_rewards = model_target(state_next_sample.to(device))\n","\n","    # compute the q-value for the next state and the action maximizing the q-value\n","    # note: the action should be valid (i.e., mask is set to 1)\n","    max_q_values = torch.max(\n","        future_rewards + action_mask_sample.to(device) * 100., # trick to select a valid action\n","        dim=1).values.detach() - 100.\n","\n","    # compute the target q-value\n","    # if the step was final, max_q_values should not be added\n","    target_q_values = rewards_sample.to(device) +  max_q_values * (1. - done_sample.to(device))\n","\n","target_q_values.shape"],"metadata":{"id":"2TdTVaNlcy5x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# It's forward propagation! Compute the Q-values for the taken actions\n","q_values = model(state_sample.to(device))\n","q_values_action = q_values.gather(dim=1, index=action_sample.to(device).unsqueeze(1)).squeeze(1)\n","q_values_action.shape"],"metadata":{"id":"PBR4CWKtc1oi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compute the loss\n","loss = loss_function(q_values_action, target_q_values)"],"metadata":{"id":"aoAEK8yfc2vL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Perform the optimization step\n","optimizer.zero_grad()\n","loss.backward()\n","optimizer.step()"],"metadata":{"id":"m5ehbLbHc5sC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for _ in range(max_episodes):\n","    state, info = env.reset()\n","    state = preprocess_state(state)\n","    action_mask = info['action_mask'].reshape((-1,))\n","    episode_reward = 0\n","\n","    for timestep in range(1, max_steps_per_episode):\n","        frame_count += 1\n","\n","        # Select an action\n","        #state_cuda = state.to(device)\n","        action = get_greedy_epsilon(model, state, action_mask)\n","        if action < 0:\n","            print(action_mask)\n","\n","        # Take the selected action\n","        state_next, reward, done, _, info = env.step((action // 8, action % 8))\n","        state_next = preprocess_state(state_next)\n","        action_mask = info['action_mask'].reshape((-1,))\n","\n","        episode_reward += reward\n","\n","        # Store the transition in the replay buffer\n","        action_history.append(action)\n","        action_mask_history.append(action_mask)\n","        state_history.append(state)\n","        state_next_history.append(state_next)\n","        rewards_history.append(reward)\n","        done_history.append(done)\n","\n","        state = state_next\n","\n","        # Update every fourth frame and once batch size is over 32\n","        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n","            update_network()\n","\n","        if frame_count % update_target_network == 0:\n","            model_target.load_state_dict(model.state_dict())\n","\n","        # Limit the state and reward history\n","        if len(rewards_history) > max_memory_length:\n","            del rewards_history[:1]\n","            del state_history[:1]\n","            del state_next_history[:1]\n","            del action_history[:1]\n","            del action_mask_history[:1]\n","            del done_history[:1]\n","\n","        if done:\n","            break\n","\n","    episode_count += 1\n","    episode_reward_history.append(episode_reward)\n","\n","    # Update running reward to check condition for solving\n","    if len(episode_reward_history) > 100:\n","        del episode_reward_history[:1]\n","    running_reward = np.mean(episode_reward_history)\n","\n","    if episode_count % 10 == 0:\n","        print(f\"Episode: {episode_count}, Frame count: {frame_count}, Running reward: {running_reward}\")\n","\n","    if episode_count % 5000 == 0:\n","        torch.save(model, 'model.{}'.format(episode_count))\n","    #if running_reward > 20:\n","    #    print(f\"Solved at episode {episode_count}!\")\n","    #    break\n","\n","\n","torch.save(model, 'model.final')"],"metadata":{"id":"s4q1Frp0c68N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time, sys\n","from IPython.display import clear_output\n","\n","board, info = env.reset()\n","state = preprocess_state(board)\n","action_mask = info['action_mask'].reshape((-1,))\n","done = False\n","env.render()\n","\n","while not done:\n","    action = get_greedy_action(model, state, action_mask)\n","    print(\"action: ({}, {})\".format(action // 8, action % 8))\n","    sys.stdout.flush()\n","\n","    time.sleep(1.0)\n","    clear_output(wait=False)\n","    board, reward, done, _, info = env.step((action // 8, action % 8))\n","    state = preprocess_state(board)\n","    action_mask = info['action_mask'].reshape((-1,))\n","    env.render()"],"metadata":{"id":"W5XaCx4Rc8vi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"WHtwbJIBc-ex"},"execution_count":null,"outputs":[]}]}